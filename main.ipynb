{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "<span style=\"font-family:Lucida Bright;\">\n",
    "<p style=\"margin-bottom:0.5cm\"></p>\n",
    "<center>\n",
    "<font size=\"8\"><b>Deep Learning, Fall 2021</b></font>\n",
    "<p style=\"margin-bottom:0.6cm\"></p>\n",
    "<font size=\"3\"><b>Final Project:</b></font>\n",
    "<p style=\"margin-bottom:0.6cm\"></p>\n",
    "<font size=\"5\"><b>Enhancing Voices for Better Speech Intelligibility</b></font>\n",
    "<p style=\"margin-bottom:2cm\"></p>\n",
    "<font size=\"6\"><b>Start</b></font>\n",
    "</center>\n",
    "<p style=\"margin-bottom:2cm\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#How-to-get-the-most-out-of-this-notebook\" data-toc-modified-id=\"How-to-get-the-most-out-of-this-notebook-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>How to get the most out of this notebook</a></span></li><li><span><a href=\"#Prerequisites\" data-toc-modified-id=\"Prerequisites-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Prerequisites</a></span></li><li><span><a href=\"#Links-and-resources\" data-toc-modified-id=\"Links-and-resources-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Links and resources</a></span></li></ul></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Initialization\" data-toc-modified-id=\"Initialization-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Initialization</a></span></li><li><span><a href=\"#Load-data\" data-toc-modified-id=\"Load-data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Load data</a></span><ul class=\"toc-item\"><li><span><a href=\"#TIMIT-dataset\" data-toc-modified-id=\"TIMIT-dataset-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>TIMIT dataset</a></span></li><li><span><a href=\"#Synthetic-speech-dataset\" data-toc-modified-id=\"Synthetic-speech-dataset-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Synthetic speech dataset</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Many people struggle to understand speech in challenging acoustic environments, such as noisy bar. Therefore, enhancing the intelligibility of noisy speech signals is one of the key challenges for any producer of modern communication devices.\n",
    "\n",
    "The problem is often tackled by dividing a noisy speech signal into a number of frequency bands and attenuating the ones where the signal-to-noise ratio is insufficient. This approach, while effective in some situations, often leads to poor results, and sometimes even exacerbates the problem it is trying to solve as the constant activation and disactivation of some of the frequency bands in response to the fluctuations in speech and noise can create a very unnatural and disturbing sounds.\n",
    "\n",
    "In this project, we will try a different approach and attempt to create a deep learning model that will produce an equalization curve that can be applied to the noisy speech signal in order to maximize its intelligibility. This will be done by running a clear speech signal through a model of human auditory processing of and searching for a combination of parameters that produce a frequency-gain curve that, when applied to the noisy signal, creates the output most similar to that of clear speech."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Official project description"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the info-doc for the Deep Learning course, the project is described as follows:\n",
    "\n",
    "> **Designing self-driving earbuds with [augmentedhearing.io](augmentedhearing.io) which enhance voices based on function correlated with speech intelligibility**\n",
    ">\n",
    "> As one in four adults struggle to understand speech in challenging acoustics we aim to train consumer earbuds to enhance voices through back propagation using DHASP model implemented using [PyTorch differentiation package](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9414571) validated on [TIMIT speech dataset](https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3) based on an objective function correlated with [HASPI speech intelligibility auditory processing model](https://www.sciencedirect.com/science/article/pii/S0167639320300431) available in Matlab."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The approach"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The time scope for the course is normed to 7 days of 9 hours, which amounts to 63 hours. The outcome should be documented in a report formatted as a [conference paper](https://drive.google.com/file/d/0BxJRy96AHCJxaUEwOFhwUExmX00/view?usp=sharing&resourcekey=0-RvwJqDVrZVijbkkifLWoYA), as well as a Jupyter notebook that ideally should recreate the main results of the report.\n",
    "\n",
    "At the beginning of the project, the following resources were available:\n",
    "\n",
    "1. speech data: a TIMIT dataset consisting of 10 sentences spoken by 630 speakers from 8 major dialect regions of the United States\n",
    "2. an [article](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9414571) describing a proposal for a differentiable objective function that can be used to train a neural network. The function is a measure of similarity of a [cepstrum](https://en.wikipedia.org/wiki/Cepstrum) of a given speech signal to the cepstrum of the target signal (usually clear, noisless speech)\n",
    "3. Matlab code for calculating the [HASPI speech intelligibility index] that can be used to evaluate the results.\n",
    "\n",
    "In turn, to create a our model, we need the following:\n",
    "\n",
    "1. clear speech audio data to use as the target for model\n",
    "2. the corresponding noisy speech audio data to train the model\n",
    "3. a PyTorch implementation of the equalization filter that can be applied to the noisy speech signal. Our model will optimize the parameters of this filter to maximize speech intelligibility\n",
    "4. a working PyTorch implementation of the objective function proposed in the [article](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9414571).\n",
    "\n",
    "Therefore, before we could start creating and tweaking our neural network, we needed to obtain the prerequisites 2 - 4, of which especially number 4: implementation of the objective function proposed in the [article](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9414571) was, as I will demonstrate later, all but straightforward.\n",
    "\n",
    "To obtain a fully functional model would require going through and understanding the project literature, obtaining the prerequisites, crating and optimizing the neural network, and documenting of the findings, which is a task that extends way beyond the 63-hour scope of this project. I have dedicated more than 3 times as much time, and concentrated my effort on obtaining a functioning implementation of the all the prerequisites necessary to build a neural network that can be used to optimize our objective. Judged by the results I will present in this notebook, I might have succeeded."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a part of project. To be able to execute all of its content:\n",
    "\n",
    "1. Download the project repository: `https://github.com/wojciechdk/deep-learning-2021-final-project.git` and run the notebook from the root.\n",
    "2. Install the necessary packages specified in the file `[project_root]/requirements.txt`. It can be done in one hook by running the command `pip install -r requirements.txt` from the command prompt from the project root.\n",
    "3. Download the [TIMIT dataset](https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3), unpack it, and place it so that the folders *DOC*, *TEST*, and *TRAIN* are placed in the folder `[project_root]/resources/data/TIMIT`.\n",
    "4. Download the Sythetic Speech dataset created by Pawel Maciej Darulewski (for permission, please contact Pawel at s200123@student.dtu.dk). Place the data from the folder containing full length sentences in the folder `[project_root]/resources/data/synthetic_speech/full_length` and the data from the folder containing 5s segments in the folder `[project_root]/resources/data/synthetic_speech/cut_5_s`.\n",
    "\n",
    "Furthermore, to fully enjoy the content, please take note of the following:\n",
    "\n",
    "- The outputs of pre-executed cells may not be rendered properly unless the notebook is **Trusted**.\n",
    "- To avoid accidental changes, most of the cells in this notebook are marked-as read only, and many are frozen (i.e. disabled from being run). To take advantage of these features, it is recommended to use the extension\n",
    "[Freeze Cell](https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions/freeze/readme.html) which works with Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The organization of the project repository"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Functions and modules"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To keep the code clean, all the functions created by the author in the course of this project are defined in the modules of the package `toolbox` residing the folder `[project_root]/toolbox`. Each module contains the functions belonging to the category indicated by the name of the module.\n",
    "\n",
    "The following modules are used in the project:\n",
    "\n",
    "- `initialization`: contains all the code that needs to be executed before anything else, such as imports of necessary packages, setting of options, definition of project paths, etc.\n",
    "- `imports`: contains the imports of all the packages needed in this project.\n",
    "- `configuration`: contains the code that defines the options regarding the appearance and interactivity of the Jupyter Notebook, Pandas, etc.\n",
    "- `paths`: contains a class containing all the paths necessary to run this project.\n",
    "- `data_loading`: contains the functions that help load the data into meaningful structures, such as the functions that load all the metadata about the TIMIT and Synthetic Speech datasets into respective Pandas dataframes.\n",
    "- `dhasp`: contains the class containing a PyTorch implementation of the Differentiable Hearing Aid Speech Processing (DHASP) model described in this [article](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9414571)\n",
    "- `dsp`: contains the functions used for processing of signals and extracting their metrics.\n",
    "- `plotting`: contains the functions that help plot the data in this project.\n",
    "- `sound`: contains the functions make it easy to listen to the audio used in this project.\n",
    "- `type_conversion`: contains the functions that facilitate the conversion between different data types, e.g. numpy and a torch.\n",
    "- `general`: contains the functions that were not given a category of their own."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test scripts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All the scripts used for testing the code in produced in this project are placed in the folder: `[project_root]/tests`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Resources"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The project's resources are placed in the folder `[project_root]/resources` and include:\n",
    "- `cache`: the cached data, such as the metadata about the TIMIT and Synthetic Speech dataset.\n",
    "- `data`: the audio files for the TIMIT and Synthetic Speech dataset.\n",
    "- `matlab`: the Matlab code containing the functions used for calculation of the [HASPI speech intelligibility auditory processing model](https://www.sciencedirect.com/science/article/pii/S0167639320300431)\n",
    "- `stoi_examples`: contains a Jupyter Notebook containing examples of how to use a Python package to calculate speech intelligibility metrics alternative to HASPI."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The entire initialization process, including:\n",
    " - imports of the necessary packages\n",
    " - configuration of the notebook and packages\n",
    " - imports of the toolbox functions\n",
    "\n",
    "is defined in the file `[project_root]/toolbox/initialization.py`. Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from toolbox.initialization import *"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Everything is now initialized and the project paths are available in the variable `paths`. Let's view one path:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "WindowsPath('G:/My Drive/DTU/Kurser/Deep_Learning_02456/final_project/resources/data/TIMIT')"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paths.data.timit.root)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load information about the audio data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "At the beginning of the project, the only audio data I had at my disposal was the [TIMIT dataset](https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3). This dataset consists of 10 sentences spoken by 630 speakers from 8 major dialect regions of the United States. However, the data contains only recordings containing clean speech, while our objective function which measures similarity between the [cepstral](https://en.wikipedia.org/wiki/Cepstrum) sequences for two audio signals, requires both clean and noisy versions of the same speech segment.\n",
    "\n",
    "Instead of generating the noisy data, I was offered by Pawel Maciej Darulewski to use a set containing samples of synthetically generated speech in different acoustical situations, which he has created for a similar project. In the following, I will therefore use Pawel's audio data. I have, however, implemented functions that allow easy access to both the TIMIT and Pawel's Synthetic Speech datasets, which I will present in the following sections.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## TIMIT dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The metadata about the TIMIT dataset is loaded using the function `load_timit_data` defined in the module `[project_root]\\toolbox\\data_loading.py`. Once loaded, the data is saved in the project cache as a Pandas dataframe. Let's load it and show a couple of rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     sentence_number data_group        dialect gender speaker  \\\n1556             202      train  North Midland      M    GAF0   \n3277             346      train       Southern      M    WSH0   \n3394             927      train  New York City      F    SGF0   \n2631               2      train       Southern      F    EAR0   \n6040               1       test        Western      M    CHH0   \n\n                      type                                               text  \\\n1556  phonetically-compact                     How much allowance do you get?   \n3277  phonetically-compact  The barracuda recoiled from the serpent's pois...   \n3394  phonetically-diverse  Naturally no woman can ever completely monopol...   \n2631               dialect       Don't ask me to carry an oily rag like that.   \n6040               dialect  She had your dark suit in greasy wash water al...   \n\n                     audio_path start_sample end_sample  \\\n1556  train\\DR3\\MGAF0\\SX202.wav            0      22221   \n3277  train\\DR5\\MWSH0\\SX346.wav            0      60314   \n3394  train\\DR6\\FSGF0\\SI927.wav            0      84788   \n2631    train\\DR5\\FEAR0\\SA2.wav            0      45978   \n6040     test\\DR7\\MCHH0\\SA1.wav            0      49562   \n\n                                             words_text  \\\n1556               [how, much, allowance, do, you, get]   \n3277  [the, barracuda, recoiled, from, the, serpent'...   \n3394  [naturally, no, woman, can, ever, completely, ...   \n2631  [don't, ask, me, to, carry, an, oily, rag, lik...   \n6040  [she, had, your, dark, suit, in, greasy, wash,...   \n\n                                     words_start_sample  \\\n1556            [1847, 3480, 6377, 12031, 13953, 15520]   \n3277  [2610, 3400, 13618, 25109, 30225, 30880, 38594...   \n3394  [2000, 16280, 19293, 25720, 28822, 32040, 4156...   \n2631  [3800, 7943, 11821, 13400, 14680, 21221, 22729...   \n6040  [2148, 6057, 9809, 11688, 16459, 20931, 23921,...   \n\n                                       words_end_sample  \\\n1556           [3480, 6377, 12031, 13953, 15520, 20772]   \n3277  [3400, 13618, 25109, 30225, 30880, 38594, 4808...   \n3394  [12680, 19293, 25720, 28822, 32040, 41560, 543...   \n2631  [7943, 11265, 13400, 14680, 21221, 22729, 2860...   \n6040  [6057, 9809, 11688, 16459, 20931, 23921, 30872...   \n\n                                          phonemes_text  \\\n1556  [h#, hh, aw, m, ah, tcl, ch, ax, l, aw, ax, n,...   \n3277  [h#, dh, ax, bcl, b, eh, er, kcl, k, ux, dx, i...   \n3394  [h#, n, ae, tcl, ch, axr, l, iy, pau, n, ow, w...   \n2631  [h#, d, ow, q, ae, s, epi, m, iy, dx, iy, kcl,...   \n6040  [h#, sh, iy, hv, eh, dcl, y, axr, dcl, d, aa, ...   \n\n                                  phonemes_start_sample  \\\n1556  [0, 1847, 2456, 3480, 3960, 5119, 5440, 6377, ...   \n3277  [0, 2610, 2820, 3400, 4820, 4940, 6475, 8320, ...   \n3394  [0, 2000, 2720, 5280, 6080, 7600, 8355, 9441, ...   \n2631  [0, 3800, 4280, 6060, 7943, 10120, 11265, 1182...   \n6040  [0, 2148, 4431, 6057, 7310, 9065, 9809, 10527,...   \n\n                                    phonemes_end_sample  \n1556  [1847, 2456, 3480, 3960, 5119, 5440, 6377, 688...  \n3277  [2610, 2820, 3400, 4820, 4940, 6475, 8320, 948...  \n3394  [2000, 2720, 5280, 6080, 7600, 8355, 9441, 126...  \n2631  [3800, 4280, 6060, 7943, 10120, 11265, 11821, ...  \n6040  [2148, 4431, 6057, 7310, 9065, 9809, 10527, 11...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence_number</th>\n      <th>data_group</th>\n      <th>dialect</th>\n      <th>gender</th>\n      <th>speaker</th>\n      <th>type</th>\n      <th>text</th>\n      <th>audio_path</th>\n      <th>start_sample</th>\n      <th>end_sample</th>\n      <th>words_text</th>\n      <th>words_start_sample</th>\n      <th>words_end_sample</th>\n      <th>phonemes_text</th>\n      <th>phonemes_start_sample</th>\n      <th>phonemes_end_sample</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1556</th>\n      <td>202</td>\n      <td>train</td>\n      <td>North Midland</td>\n      <td>M</td>\n      <td>GAF0</td>\n      <td>phonetically-compact</td>\n      <td>How much allowance do you get?</td>\n      <td>train\\DR3\\MGAF0\\SX202.wav</td>\n      <td>0</td>\n      <td>22221</td>\n      <td>[how, much, allowance, do, you, get]</td>\n      <td>[1847, 3480, 6377, 12031, 13953, 15520]</td>\n      <td>[3480, 6377, 12031, 13953, 15520, 20772]</td>\n      <td>[h#, hh, aw, m, ah, tcl, ch, ax, l, aw, ax, n,...</td>\n      <td>[0, 1847, 2456, 3480, 3960, 5119, 5440, 6377, ...</td>\n      <td>[1847, 2456, 3480, 3960, 5119, 5440, 6377, 688...</td>\n    </tr>\n    <tr>\n      <th>3277</th>\n      <td>346</td>\n      <td>train</td>\n      <td>Southern</td>\n      <td>M</td>\n      <td>WSH0</td>\n      <td>phonetically-compact</td>\n      <td>The barracuda recoiled from the serpent's pois...</td>\n      <td>train\\DR5\\MWSH0\\SX346.wav</td>\n      <td>0</td>\n      <td>60314</td>\n      <td>[the, barracuda, recoiled, from, the, serpent'...</td>\n      <td>[2610, 3400, 13618, 25109, 30225, 30880, 38594...</td>\n      <td>[3400, 13618, 25109, 30225, 30880, 38594, 4808...</td>\n      <td>[h#, dh, ax, bcl, b, eh, er, kcl, k, ux, dx, i...</td>\n      <td>[0, 2610, 2820, 3400, 4820, 4940, 6475, 8320, ...</td>\n      <td>[2610, 2820, 3400, 4820, 4940, 6475, 8320, 948...</td>\n    </tr>\n    <tr>\n      <th>3394</th>\n      <td>927</td>\n      <td>train</td>\n      <td>New York City</td>\n      <td>F</td>\n      <td>SGF0</td>\n      <td>phonetically-diverse</td>\n      <td>Naturally no woman can ever completely monopol...</td>\n      <td>train\\DR6\\FSGF0\\SI927.wav</td>\n      <td>0</td>\n      <td>84788</td>\n      <td>[naturally, no, woman, can, ever, completely, ...</td>\n      <td>[2000, 16280, 19293, 25720, 28822, 32040, 4156...</td>\n      <td>[12680, 19293, 25720, 28822, 32040, 41560, 543...</td>\n      <td>[h#, n, ae, tcl, ch, axr, l, iy, pau, n, ow, w...</td>\n      <td>[0, 2000, 2720, 5280, 6080, 7600, 8355, 9441, ...</td>\n      <td>[2000, 2720, 5280, 6080, 7600, 8355, 9441, 126...</td>\n    </tr>\n    <tr>\n      <th>2631</th>\n      <td>2</td>\n      <td>train</td>\n      <td>Southern</td>\n      <td>F</td>\n      <td>EAR0</td>\n      <td>dialect</td>\n      <td>Don't ask me to carry an oily rag like that.</td>\n      <td>train\\DR5\\FEAR0\\SA2.wav</td>\n      <td>0</td>\n      <td>45978</td>\n      <td>[don't, ask, me, to, carry, an, oily, rag, lik...</td>\n      <td>[3800, 7943, 11821, 13400, 14680, 21221, 22729...</td>\n      <td>[7943, 11265, 13400, 14680, 21221, 22729, 2860...</td>\n      <td>[h#, d, ow, q, ae, s, epi, m, iy, dx, iy, kcl,...</td>\n      <td>[0, 3800, 4280, 6060, 7943, 10120, 11265, 1182...</td>\n      <td>[3800, 4280, 6060, 7943, 10120, 11265, 11821, ...</td>\n    </tr>\n    <tr>\n      <th>6040</th>\n      <td>1</td>\n      <td>test</td>\n      <td>Western</td>\n      <td>M</td>\n      <td>CHH0</td>\n      <td>dialect</td>\n      <td>She had your dark suit in greasy wash water al...</td>\n      <td>test\\DR7\\MCHH0\\SA1.wav</td>\n      <td>0</td>\n      <td>49562</td>\n      <td>[she, had, your, dark, suit, in, greasy, wash,...</td>\n      <td>[2148, 6057, 9809, 11688, 16459, 20931, 23921,...</td>\n      <td>[6057, 9809, 11688, 16459, 20931, 23921, 30872...</td>\n      <td>[h#, sh, iy, hv, eh, dcl, y, axr, dcl, d, aa, ...</td>\n      <td>[0, 2148, 4431, 6057, 7310, 9065, 9809, 10527,...</td>\n      <td>[2148, 4431, 6057, 7310, 9065, 9809, 10527, 11...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the TIMIT meta data from the cache.\n",
    "df_timit = pd.read_pickle(paths.cache.df_timit)\n",
    "\n",
    "# Show the top 5 rows\n",
    "display(df_timit.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, let's play one sentence from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t.sound.play_timit(df_timit.loc[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Synthetic speech dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The metadata about the dataset containing synthetic speech in different audio settings is loaded using the function `load_synthetic_speech_data` defined in the module `toolbox\\data_loading.py`. Once loaded, the data is saved in the project cache as a Pandas dataframe. Let's load it and show a couple of rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     speaker length         variant segment fullness clarity  \\\n627   joanna     5s       distorted      27      NaN     NaN   \n119      ivy     5s  zoom_augmented      27        4       1   \n1103    joey     5s              tv       1      NaN     NaN   \n1504  justin     5s              tv      23      NaN     NaN   \n333      ivy     5s            zoom       8      NaN     NaN   \n\n                      audio_path  \n627   cut_5_s\\joanna\\bitc\\27.wav  \n119       cut_5_s\\ivy\\4-1\\27.wav  \n1103     cut_5_s\\joey\\tele\\1.wav  \n1504  cut_5_s\\justin\\tele\\23.wav  \n333       cut_5_s\\ivy\\zoom\\8.wav  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>speaker</th>\n      <th>length</th>\n      <th>variant</th>\n      <th>segment</th>\n      <th>fullness</th>\n      <th>clarity</th>\n      <th>audio_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>627</th>\n      <td>joanna</td>\n      <td>5s</td>\n      <td>distorted</td>\n      <td>27</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>cut_5_s\\joanna\\bitc\\27.wav</td>\n    </tr>\n    <tr>\n      <th>119</th>\n      <td>ivy</td>\n      <td>5s</td>\n      <td>zoom_augmented</td>\n      <td>27</td>\n      <td>4</td>\n      <td>1</td>\n      <td>cut_5_s\\ivy\\4-1\\27.wav</td>\n    </tr>\n    <tr>\n      <th>1103</th>\n      <td>joey</td>\n      <td>5s</td>\n      <td>tv</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>cut_5_s\\joey\\tele\\1.wav</td>\n    </tr>\n    <tr>\n      <th>1504</th>\n      <td>justin</td>\n      <td>5s</td>\n      <td>tv</td>\n      <td>23</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>cut_5_s\\justin\\tele\\23.wav</td>\n    </tr>\n    <tr>\n      <th>333</th>\n      <td>ivy</td>\n      <td>5s</td>\n      <td>zoom</td>\n      <td>8</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>cut_5_s\\ivy\\zoom\\8.wav</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the meta data about the synthetic speech dataset from the cache.\n",
    "df_synthetic_speech = pd.read_pickle(paths.cache.df_synthetic_speech)\n",
    "\n",
    "# Show the top 5 rows.\n",
    "display(df_synthetic_speech.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's play one of the first 5s segments from the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "    speaker length variant segment fullness clarity  \\\n559  joanna     5s  babble      10      NaN     NaN   \n\n                     audio_path  \n559  cut_5_s\\joanna\\babb\\10.wav  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>speaker</th>\n      <th>length</th>\n      <th>variant</th>\n      <th>segment</th>\n      <th>fullness</th>\n      <th>clarity</th>\n      <th>audio_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>559</th>\n      <td>joanna</td>\n      <td>5s</td>\n      <td>babble</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>cut_5_s\\joanna\\babb\\10.wav</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the properties of the file to play.\n",
    "mask = (\n",
    "    (df_synthetic_speech['speaker'] == 'joanna')\n",
    "    & (df_synthetic_speech['length'] == '5s')\n",
    "    & (df_synthetic_speech['segment'] == 10)\n",
    "    & (df_synthetic_speech['variant'] == 'babble')\n",
    ")\n",
    "\n",
    "# Show the file data.\n",
    "display(\n",
    "    df_synthetic_speech.loc[mask, :]\n",
    ")\n",
    "\n",
    "# Play.\n",
    "t.sound.play_synthetic_speech(df_synthetic_speech.loc[mask, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Implementation of the DHASP model as the objective function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is a declared goal of the project to base the objective function of the model on the [Differentiable Hearing Aid Speech Processing (DHASP)](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9414571) framework. This framework has been developed to optimize the signal processing in a hearing aid so that the signal perceived by a hearing-impaired person would be as close as possibe to that perceived by a person with normal hearing.\n",
    "\n",
    "<center>\n",
    "<img src=\"resources/graphics/dhasp_original_framework.jpg\"\n",
    "alt=\"DHASP framework as proposed by its creators\"\n",
    "title=\"Original DHASP framework\"\n",
    "width=\"600\"/>\n",
    "</center>\n",
    "\n",
    "The original framework compares therefore the output of auditory processing of a signal by a person with normal hearing to the output of the processing of **the same** signal by a person with impaired hearing (figure XXX). We, on the other hand, would like to alter this framework so that iompares the result of auditory processing of two **different** signals - noisy and noise-free - by the same, normal hearing person (Figure XXX). When constructed this way, the framework can tweak the equalization of the noisy signal so that it resembles its noise-free counterpart to a highest possible degree.\n",
    "\n",
    "<center>\n",
    "<br>\n",
    "<img src=\"resources/graphics/dhasp_proposed_framework.jpg\"\n",
    "alt=\"DHASP framework as proposed by its creators\"\n",
    "title=\"Original DHASP framework\"\n",
    "width=\"600\"/>\n",
    "</center>\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "\\underset{G}{\\text{argmin}} L \\left( f(G, x), y \\right)\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# End"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "PyCharm (Exercises)",
   "language": "python",
   "name": "pycharm-f0629d26"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "445px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "093e89bffde74c5a8863a1d8ffe0bb66": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "1a5365fd770e464891fe02b94cb34b95": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1c7a498865af41a1bd4af63eca07bd28": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatTextModel",
      "state": {
       "layout": "IPY_MODEL_68759d18977d4757ba3eb3cf81aa1129",
       "step": null,
       "style": "IPY_MODEL_feea9000784940f3b601fd1ba613dd31"
      }
     },
     "1ee2806f2e314951b8b42666540f6aee": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "371339a327cc48d8a56358f512674975": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "65604f719f9e41f3b6d495d3aa907529": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "68759d18977d4757ba3eb3cf81aa1129": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "68dd48c190fe4b8c92790dc65803d785": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DropdownModel",
      "state": {
       "_options_labels": [
        "One",
        "Two",
        "Three"
       ],
       "description": "Number:",
       "index": 1,
       "layout": "IPY_MODEL_371339a327cc48d8a56358f512674975",
       "style": "IPY_MODEL_89e645fe88da40b2890bc1c08b50be78"
      }
     },
     "839595af423541ef946ce08e437e5b7e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "89e645fe88da40b2890bc1c08b50be78": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "9091258d01434bf8ac0f41c12363e6a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatTextModel",
      "state": {
       "layout": "IPY_MODEL_c22c18c67fe24f06ada7d28e3cb6ff90",
       "step": null,
       "style": "IPY_MODEL_839595af423541ef946ce08e437e5b7e"
      }
     },
     "afadc0b9a4f3458685319c1818955895": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DropdownModel",
      "state": {
       "_options_labels": [
        "One",
        "Two",
        "Three"
       ],
       "description": "Number:",
       "index": 1,
       "layout": "IPY_MODEL_1ee2806f2e314951b8b42666540f6aee",
       "style": "IPY_MODEL_093e89bffde74c5a8863a1d8ffe0bb66"
      }
     },
     "c22c18c67fe24f06ada7d28e3cb6ff90": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c64dd728486c4d4396b6b2b4a02ca054": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatSliderModel",
      "state": {
       "layout": "IPY_MODEL_1a5365fd770e464891fe02b94cb34b95",
       "step": 0.1,
       "style": "IPY_MODEL_ff2f617f26294276834c51733d0f253c"
      }
     },
     "d7c60be1aeb643fe9d80a622e61b4cc2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fe21ecb1be5e431c9745cf04421dc9a0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatSliderModel",
      "state": {
       "layout": "IPY_MODEL_d7c60be1aeb643fe9d80a622e61b4cc2",
       "step": 0.1,
       "style": "IPY_MODEL_65604f719f9e41f3b6d495d3aa907529",
       "value": 55.4
      }
     },
     "feea9000784940f3b601fd1ba613dd31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "ff2f617f26294276834c51733d0f253c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}