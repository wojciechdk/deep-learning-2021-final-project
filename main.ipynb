{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "<span style=\"font-family:Lucida Bright;\">\n",
    "<p style=\"margin-bottom:0.5cm\"></p>\n",
    "<center>\n",
    "<font size=\"8\"><b>Deep Learning, Fall 2021</b></font>\n",
    "<p style=\"margin-bottom:0.6cm\"></p>\n",
    "<font size=\"3\"><b>Final Project:</b></font>\n",
    "<p style=\"margin-bottom:0.6cm\"></p>\n",
    "<font size=\"5\"><b>Enhancing Voices for Better Speech Intelligibility</b></font>\n",
    "<p style=\"margin-bottom:2cm\"></p>\n",
    "<font size=\"6\"><b>Start</b></font>\n",
    "</center>\n",
    "<p style=\"margin-bottom:2cm\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#How-to-get-the-most-out-of-this-notebook\" data-toc-modified-id=\"How-to-get-the-most-out-of-this-notebook-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>How to get the most out of this notebook</a></span></li><li><span><a href=\"#Prerequisites\" data-toc-modified-id=\"Prerequisites-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Prerequisites</a></span></li><li><span><a href=\"#Links-and-resources\" data-toc-modified-id=\"Links-and-resources-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Links and resources</a></span></li></ul></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Initialization\" data-toc-modified-id=\"Initialization-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Initialization</a></span></li><li><span><a href=\"#Load-data\" data-toc-modified-id=\"Load-data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Load data</a></span><ul class=\"toc-item\"><li><span><a href=\"#TIMIT-dataset\" data-toc-modified-id=\"TIMIT-dataset-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>TIMIT dataset</a></span></li><li><span><a href=\"#Synthetic-speech-dataset\" data-toc-modified-id=\"Synthetic-speech-dataset-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Synthetic speech dataset</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Many people struggle to understand speech in challenging acoustic environments, such as noisy bar. Therefore, enhancing the intelligibility of noisy speech signals is one of the key challenges for any producer of modern communication devices.\n",
    "\n",
    "The problem is often tackled by dividing a noisy speech signal into a number of frequency bands and attenuating the ones where the signal-to-noise ratio is insufficient. This approach, while effective in some situations, often leads to poor results, and sometimes even exacerbates the problem it is trying to solve as the constant activation and disactivation of some of the frequency bands in response to the fluctuations in speech and noise can create a very unnatural and disturbing sounds.\n",
    "\n",
    "In this project, we will try a different approach and attempt to create a deep learning model that will produce an equalization curve that can be applied to the noisy speech signal in order to maximize its intelligibility. This will be done by running a clear speech signal through a model of human auditory processing of and searching for a combination of parameters that produce a frequency-gain curve that, when applied to the noisy signal, creates the output most similar to that of clear speech."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Official project description"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the info-doc for the Deep Learning course, the project is described as follows:\n",
    "\n",
    "> **Designing self-driving earbuds with [augmentedhearing.io](augmentedhearing.io) which enhance voices based on function correlated with speech intelligibility**\n",
    ">\n",
    "> As one in four adults struggle to understand speech in challenging acoustics we aim to train consumer earbuds to enhance voices through back propagation using DHASP model implemented using [PyTorch differentiation package](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9414571) validated on [TIMIT speech dataset](https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3) based on an objective function correlated with [HASPI speech intelligibility auditory processing model](https://www.sciencedirect.com/science/article/pii/S0167639320300431) available in Matlab."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The approach"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The time scope for the course is normed to 7 days of 9 hours, which amounts to 63 hours. The outcome should be documented in a report formatted as a [conference paper](https://drive.google.com/file/d/0BxJRy96AHCJxaUEwOFhwUExmX00/view?usp=sharing&resourcekey=0-RvwJqDVrZVijbkkifLWoYA), as well as a Jupyter notebook that ideally should recreate the main results of the report.\n",
    "\n",
    "At the beginning of the project, the following resources were available:\n",
    "\n",
    "1. speech data: a TIMIT dataset consisting of 10 sentences spoken by 630 speakers from 8 major dialect regions of the United States\n",
    "2. an [article](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9414571) describing a proposal for a differentiable objective function that can be used to train a neural network. The function is a measure of similarity of a [cepstrum](https://en.wikipedia.org/wiki/Cepstrum) of a given speech signal to the cepstrum of the target signal (usually clear, noisless speech)\n",
    "3. Matlab code for calculating the [HASPI speech intelligibility index] that can be used to evaluate the results.\n",
    "\n",
    "In turn, to create a our model, we need the following:\n",
    "\n",
    "1. clear speech audio data to use as the target for model\n",
    "2. the corresponding noisy speech audio data to train the model\n",
    "3. a PyTorch implementation of the equalization filter that can be applied to the noisy speech signal. Our model will optimize the parameters of this filter to maximize speech intelligibility\n",
    "4. a working PyTorch implementation of the objective function proposed in the [article](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9414571).\n",
    "\n",
    "Therefore, before we could start creating and tweaking our neural network, we needed to obtain the prerequisites 2 - 4, of which especially number 4: implementation of the objective function proposed in the [article](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9414571) was, as I will demonstrate later, all but straightforward.\n",
    "\n",
    "To obtain a fully functional model would require going through and understanding the project literature, obtaining the prerequisites, crating and optimizing the neural network, and documenting of the findings, which is a task that extends way beyond the 63-hour scope of this project. I have dedicated more than 3 times as much time, and concentrated my effort on obtaining a functioning implementation of the all the prerequisites necessary to build a neural network that can be used to optimize our objective. Judged by the results I will present in this notebook, I might have succeeded."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a part of project. To be able to execute all of its content:\n",
    "\n",
    "1. Download the project repository: `https://github.com/wojciechdk/deep-learning-2021-final-project.git` and run the notebook from the root.\n",
    "2. Install the necessary packages specified in the file `[project_root]/requirements.txt`. It can be done in one hook by running the command `pip install -r requirements.txt` from the command prompt from the project root.\n",
    "3. Download the [TIMIT dataset](https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3), unpack it, and place it so that the folders *DOC*, *TEST*, and *TRAIN* are placed in the folder `[project_root]/resources/data/TIMIT`.\n",
    "4. Download the Sythetic Speech dataset created by Pawel Maciej Darulewski (for permission, please contact Pawel at s200123@student.dtu.dk). Place the data from the folder containing full length sentences in the folder `[project_root]/resources/data/synthetic_speech/full_length` and the data from the folder containing 5s segments in the folder `[project_root]/resources/data/synthetic_speech/cut_5_s`.\n",
    "\n",
    "Furthermore, to fully enjoy the content, please take note of the following:\n",
    "\n",
    "- The outputs of pre-executed cells may not be rendered properly unless the notebook is **Trusted**.\n",
    "- To avoid accidental changes, most of the cells in this notebook are marked-as read only, and many are frozen (i.e. disabled from being run). To take advantage of these features, it is recommended to use the extension\n",
    "[Freeze Cell](https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions/freeze/readme.html) which works with Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The organization of the project repository"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Functions and modules"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To keep the code clean, all the functions created by the author in the course of the project are defined in modules of the package `toolbox` residing the folder `[project root]/toolbox`. Each module contains the functions belonging to the category indicated by the name of the module.\n",
    "\n",
    "The following modules are used in the project:\n",
    "\n",
    "- `initialization`: contains all the code that needs to be executed before anything else, such as imports of necessary packages, setting of options, definition of project paths, etc.\n",
    "- `imports`: contains the imports of all the packages needed in this project.\n",
    "- `configuration`: contains the code that defines the options regarding the appearance and interactivity of the Jupyter Notebook, Pandas, etc.\n",
    "- `paths`: contains a class containing all the paths necessary to run this project.\n",
    "- `data_loading`: contains the functions that help load the data into meaningful structures, such as the functions that load all the metadata about the TIMIT and Synthetic Speech datasets into respective Pandas dataframes.\n",
    "- `dhasp`: contains the class containing a PyTorch implementation of the Differentiable Hearing Aid Speech Processing model described in this [article](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9414571)\n",
    "- `dsp`: contains the functions used for processing of signals and extracting their metrics.\n",
    "- `plotting`: contains the functions that help plot the data in this project.\n",
    "- `sound`: contains the functions make it easy to listen to the audio used in this project.\n",
    "- `type_conversion`: contains the functions that facilitate the conversion between different data types, e.g. numpy and a torch.\n",
    "- `general`: contains the functions that were not given a category of their own."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test scripts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All the scripts used for testing the code in produced in this project are placed in the folder: `[project_root]/tests`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Resources"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The project's resources are placed in the folder `[project_root]/resources` and include:\n",
    "- `cache`: the cached data, such as the metadata about the TIMIT and Synthetic Speech dataset.\n",
    "- `data`: the audio files for the TIMIT and Synthetic Speech dataset.\n",
    "- `matlab`: the Matlab code containing the functions used for calculation of the [HASPI speech intelligibility auditory processing model](https://www.sciencedirect.com/science/article/pii/S0167639320300431)\n",
    "- `stoi_examples`: contains a Jupyter Notebook containing examples of how to use a Python package to calculate speech intelligibility metrics alternative to HASPI."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The entire initialization process, including:\n",
    " - imports of the necessary packages\n",
    " - configuration of the notebook and packages\n",
    " - imports of the toolbox functions\n",
    "\n",
    "is defined in the file `[project_root]/toolbox/initialization.py`. Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from toolbox.initialization import *"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Everything is now initialized and the project paths are available in the variable `paths`. Let's view one path:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "WindowsPath('G:/My Drive/DTU/Kurser/Deep_Learning_02456/final_project/resources/data/TIMIT')"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paths.data.timit.root)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load information about the audio data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "At the beginning of the project, the only audio data I had at my disposal was the [TIMIT dataset](https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3). This dataset consists of 10 sentences spoken by 630 speakers from 8 major dialect regions of the United States. However, the data contains only recordings containing clean speech, while our objective function, which measures similarity between the [cepstral](https://en.wikipedia.org/wiki/Cepstrum) sequences for two audio signals, requires both the clean and noisy recordings.\n",
    "\n",
    "Instead of generating the noisy data, I was offered by Pawel Maciej Darulewski to use a set containing samples of synthetically generated speech in different acoustical situations which he has created for a similar project. In the following, I will therefore use Pawel's audio data. I have, however, implemented procedures that allow easy access to both datasets, which I will present in the following sections.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## TIMIT dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The metadata about the TIMIT dataset is loaded using the function `load_timit_data` defined in the module `[project_root]\\toolbox\\data_loading.py`. Once loaded, the data is saved in the project cache as a Pandas dataframe. Let's load it and show a couple of rows:"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "\\underset{G}{\\text{argmin}} L \\left( f(G, x), y \\right)\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     sentence_number data_group        dialect gender speaker  \\\n4633            2104       test    New England      F    DAC1   \n3902            1121      train        Western      M    CRE0   \n335              112      train    New England      M    TJS0   \n5650               1       test       Southern      F    MAH0   \n1155             121      train  North Midland      F    CKE0   \n\n                      type                                               text  \\\n4633  phonetically-diverse         Ahah, he thought, a lush divorcee at last.   \n3902  phonetically-diverse  In either case, they do not appreciate the pri...   \n335   phonetically-compact        Tugboats are capable of hauling huge loads.   \n5650               dialect  She had your dark suit in greasy wash water al...   \n1155  phonetically-compact   Trespassing is forbidden and subject to penalty.   \n\n                      audio_path start_sample end_sample  \\\n4633   test\\DR1\\FDAC1\\SI2104.wav            0      73626   \n3902  train\\DR7\\MCRE0\\SI1121.wav            0      58881   \n335    train\\DR1\\MTJS0\\SX112.wav            0      53658   \n5650      test\\DR5\\FMAH0\\SA1.wav            0      52941   \n1155   train\\DR3\\FCKE0\\SX121.wav            0      40449   \n\n                                             words_text  \\\n4633   [ahah, he, thought, a, lush, divorcee, at, last]   \n3902  [in, either, case, they, do, not, appreciate, ...   \n335   [tugboats, are, capable, of, hauling, huge, lo...   \n5650  [she, had, your, dark, suit, in, greasy, wash,...   \n1155  [trespassing, is, forbidden, and, subject, to,...   \n\n                                     words_start_sample  \\\n4633  [2680, 12520, 16120, 25107, 28323, 36060, 5666...   \n3902  [2520, 3960, 7800, 18172, 20040, 21840, 24106,...   \n335   [13451, 21080, 21880, 29880, 31207, 38015, 43434]   \n5650  [2339, 4845, 8620, 9817, 14788, 20280, 22826, ...   \n1155   [2280, 11400, 13680, 21320, 23640, 30120, 31240]   \n\n                                       words_end_sample  \\\n4633  [12520, 16120, 25107, 27697, 36060, 51386, 599...   \n3902  [3960, 7800, 15999, 20040, 21840, 24106, 34046...   \n335   [21080, 21880, 29880, 31207, 38015, 43434, 51311]   \n5650  [4845, 8980, 9817, 14788, 20280, 22826, 29159,...   \n1155  [11400, 13680, 21320, 23640, 30760, 31240, 38520]   \n\n                                          phonemes_text  \\\n4633  [h#, aa, hv, aa, hh, iy, th, ao, tcl, q, ey, e...   \n3902  [h#, ix, n, iy, dh, er, kcl, k, ey, s, pau, dh...   \n335   [h#, t, ah, gcl, b, ow, tcl, s, ix, kcl, k, ey...   \n5650  [h#, sh, iy, hv, ae, dcl, jh, axr, dcl, d, aa,...   \n1155  [h#, t, r, eh, s, pcl, p, ae, s, ih, ng, ix, z...   \n\n                                  phonemes_start_sample  \\\n4633  [0, 2680, 5013, 6480, 12520, 14560, 16120, 189...   \n3902  [0, 2520, 3000, 3960, 5640, 6200, 7800, 8680, ...   \n335   [3480, 13451, 14078, 15480, 17160, 17295, 1916...   \n5650  [0, 2339, 4060, 4845, 5840, 7944, 8620, 8980, ...   \n1155  [0, 2280, 3400, 3853, 4760, 5848, 6224, 6760, ...   \n\n                                    phonemes_end_sample  \n4633  [2680, 5013, 6480, 12520, 14560, 16120, 18900,...  \n3902  [2520, 3000, 3960, 5640, 6200, 7800, 8680, 964...  \n335   [13451, 14078, 15480, 17160, 17295, 19160, 200...  \n5650  [2339, 4060, 4845, 5840, 7944, 8620, 8980, 981...  \n1155  [2280, 3400, 3853, 4760, 5848, 6224, 6760, 876...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence_number</th>\n      <th>data_group</th>\n      <th>dialect</th>\n      <th>gender</th>\n      <th>speaker</th>\n      <th>type</th>\n      <th>text</th>\n      <th>audio_path</th>\n      <th>start_sample</th>\n      <th>end_sample</th>\n      <th>words_text</th>\n      <th>words_start_sample</th>\n      <th>words_end_sample</th>\n      <th>phonemes_text</th>\n      <th>phonemes_start_sample</th>\n      <th>phonemes_end_sample</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4633</th>\n      <td>2104</td>\n      <td>test</td>\n      <td>New England</td>\n      <td>F</td>\n      <td>DAC1</td>\n      <td>phonetically-diverse</td>\n      <td>Ahah, he thought, a lush divorcee at last.</td>\n      <td>test\\DR1\\FDAC1\\SI2104.wav</td>\n      <td>0</td>\n      <td>73626</td>\n      <td>[ahah, he, thought, a, lush, divorcee, at, last]</td>\n      <td>[2680, 12520, 16120, 25107, 28323, 36060, 5666...</td>\n      <td>[12520, 16120, 25107, 27697, 36060, 51386, 599...</td>\n      <td>[h#, aa, hv, aa, hh, iy, th, ao, tcl, q, ey, e...</td>\n      <td>[0, 2680, 5013, 6480, 12520, 14560, 16120, 189...</td>\n      <td>[2680, 5013, 6480, 12520, 14560, 16120, 18900,...</td>\n    </tr>\n    <tr>\n      <th>3902</th>\n      <td>1121</td>\n      <td>train</td>\n      <td>Western</td>\n      <td>M</td>\n      <td>CRE0</td>\n      <td>phonetically-diverse</td>\n      <td>In either case, they do not appreciate the pri...</td>\n      <td>train\\DR7\\MCRE0\\SI1121.wav</td>\n      <td>0</td>\n      <td>58881</td>\n      <td>[in, either, case, they, do, not, appreciate, ...</td>\n      <td>[2520, 3960, 7800, 18172, 20040, 21840, 24106,...</td>\n      <td>[3960, 7800, 15999, 20040, 21840, 24106, 34046...</td>\n      <td>[h#, ix, n, iy, dh, er, kcl, k, ey, s, pau, dh...</td>\n      <td>[0, 2520, 3000, 3960, 5640, 6200, 7800, 8680, ...</td>\n      <td>[2520, 3000, 3960, 5640, 6200, 7800, 8680, 964...</td>\n    </tr>\n    <tr>\n      <th>335</th>\n      <td>112</td>\n      <td>train</td>\n      <td>New England</td>\n      <td>M</td>\n      <td>TJS0</td>\n      <td>phonetically-compact</td>\n      <td>Tugboats are capable of hauling huge loads.</td>\n      <td>train\\DR1\\MTJS0\\SX112.wav</td>\n      <td>0</td>\n      <td>53658</td>\n      <td>[tugboats, are, capable, of, hauling, huge, lo...</td>\n      <td>[13451, 21080, 21880, 29880, 31207, 38015, 43434]</td>\n      <td>[21080, 21880, 29880, 31207, 38015, 43434, 51311]</td>\n      <td>[h#, t, ah, gcl, b, ow, tcl, s, ix, kcl, k, ey...</td>\n      <td>[3480, 13451, 14078, 15480, 17160, 17295, 1916...</td>\n      <td>[13451, 14078, 15480, 17160, 17295, 19160, 200...</td>\n    </tr>\n    <tr>\n      <th>5650</th>\n      <td>1</td>\n      <td>test</td>\n      <td>Southern</td>\n      <td>F</td>\n      <td>MAH0</td>\n      <td>dialect</td>\n      <td>She had your dark suit in greasy wash water al...</td>\n      <td>test\\DR5\\FMAH0\\SA1.wav</td>\n      <td>0</td>\n      <td>52941</td>\n      <td>[she, had, your, dark, suit, in, greasy, wash,...</td>\n      <td>[2339, 4845, 8620, 9817, 14788, 20280, 22826, ...</td>\n      <td>[4845, 8980, 9817, 14788, 20280, 22826, 29159,...</td>\n      <td>[h#, sh, iy, hv, ae, dcl, jh, axr, dcl, d, aa,...</td>\n      <td>[0, 2339, 4060, 4845, 5840, 7944, 8620, 8980, ...</td>\n      <td>[2339, 4060, 4845, 5840, 7944, 8620, 8980, 981...</td>\n    </tr>\n    <tr>\n      <th>1155</th>\n      <td>121</td>\n      <td>train</td>\n      <td>North Midland</td>\n      <td>F</td>\n      <td>CKE0</td>\n      <td>phonetically-compact</td>\n      <td>Trespassing is forbidden and subject to penalty.</td>\n      <td>train\\DR3\\FCKE0\\SX121.wav</td>\n      <td>0</td>\n      <td>40449</td>\n      <td>[trespassing, is, forbidden, and, subject, to,...</td>\n      <td>[2280, 11400, 13680, 21320, 23640, 30120, 31240]</td>\n      <td>[11400, 13680, 21320, 23640, 30760, 31240, 38520]</td>\n      <td>[h#, t, r, eh, s, pcl, p, ae, s, ih, ng, ix, z...</td>\n      <td>[0, 2280, 3400, 3853, 4760, 5848, 6224, 6760, ...</td>\n      <td>[2280, 3400, 3853, 4760, 5848, 6224, 6760, 876...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the TIMIT meta data from the cache.\n",
    "df_timit = pd.read_pickle(paths.cache.df_timit)\n",
    "\n",
    "# Show the top 5 rows\n",
    "display(df_timit.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, let's play one sentence from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t.sound.play_timit(df_timit.loc[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Synthetic speech dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The metadata about the dataset containing synthetic speech in different audio settings is loaded using the function `load_synthetic_speech_data` defined in the module `toolbox\\data_loading.py`. Once loaded, the data is saved in the project cache as a Pandas dataframe. Let's load it and show a couple of rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      speaker length         variant segment fullness clarity  \\\n3036  matthew     5s           music      27      NaN     NaN   \n3406    salli     5s           music       2      NaN     NaN   \n2272    kevin     5s              tv      15      NaN     NaN   \n2819  matthew     5s  zoom_augmented      33        1       1   \n727    joanna     5s            zoom      22      NaN     NaN   \n\n                       audio_path  \n3036  cut_5_s\\matthew\\musi\\27.wav  \n3406     cut_5_s\\salli\\musi\\2.wav  \n2272    cut_5_s\\kevin\\tele\\15.wav  \n2819   cut_5_s\\matthew\\1-1\\33.wav  \n727    cut_5_s\\joanna\\zoom\\22.wav  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>speaker</th>\n      <th>length</th>\n      <th>variant</th>\n      <th>segment</th>\n      <th>fullness</th>\n      <th>clarity</th>\n      <th>audio_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3036</th>\n      <td>matthew</td>\n      <td>5s</td>\n      <td>music</td>\n      <td>27</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>cut_5_s\\matthew\\musi\\27.wav</td>\n    </tr>\n    <tr>\n      <th>3406</th>\n      <td>salli</td>\n      <td>5s</td>\n      <td>music</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>cut_5_s\\salli\\musi\\2.wav</td>\n    </tr>\n    <tr>\n      <th>2272</th>\n      <td>kevin</td>\n      <td>5s</td>\n      <td>tv</td>\n      <td>15</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>cut_5_s\\kevin\\tele\\15.wav</td>\n    </tr>\n    <tr>\n      <th>2819</th>\n      <td>matthew</td>\n      <td>5s</td>\n      <td>zoom_augmented</td>\n      <td>33</td>\n      <td>1</td>\n      <td>1</td>\n      <td>cut_5_s\\matthew\\1-1\\33.wav</td>\n    </tr>\n    <tr>\n      <th>727</th>\n      <td>joanna</td>\n      <td>5s</td>\n      <td>zoom</td>\n      <td>22</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>cut_5_s\\joanna\\zoom\\22.wav</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the meta data about the synthetic speech dataset from the cache.\n",
    "df_synthetic_speech = pd.read_pickle(paths.cache.df_synthetic_speech)\n",
    "\n",
    "# Show the top 5 rows.\n",
    "display(df_synthetic_speech.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's play one of the first 5s segments from the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "    speaker length variant segment fullness clarity  \\\n559  joanna     5s  babble      10      NaN     NaN   \n\n                     audio_path  \n559  cut_5_s\\joanna\\babb\\10.wav  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>speaker</th>\n      <th>length</th>\n      <th>variant</th>\n      <th>segment</th>\n      <th>fullness</th>\n      <th>clarity</th>\n      <th>audio_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>559</th>\n      <td>joanna</td>\n      <td>5s</td>\n      <td>babble</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>cut_5_s\\joanna\\babb\\10.wav</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the properties of the file to play.\n",
    "mask = (\n",
    "    (df_synthetic_speech['speaker'] == 'joanna')\n",
    "    & (df_synthetic_speech['length'] == '5s')\n",
    "    & (df_synthetic_speech['segment'] == 10)\n",
    "    & (df_synthetic_speech['variant'] == 'babble')\n",
    ")\n",
    "\n",
    "# Show the file data.\n",
    "display(\n",
    "    df_synthetic_speech.loc[mask, :]\n",
    ")\n",
    "\n",
    "# Play.\n",
    "t.sound.play_synthetic_speech(df_synthetic_speech.loc[mask, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# dsaf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# End"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "PyCharm (Exercises)",
   "language": "python",
   "name": "pycharm-f0629d26"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "445px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "093e89bffde74c5a8863a1d8ffe0bb66": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "1a5365fd770e464891fe02b94cb34b95": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1c7a498865af41a1bd4af63eca07bd28": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatTextModel",
      "state": {
       "layout": "IPY_MODEL_68759d18977d4757ba3eb3cf81aa1129",
       "step": null,
       "style": "IPY_MODEL_feea9000784940f3b601fd1ba613dd31"
      }
     },
     "1ee2806f2e314951b8b42666540f6aee": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "371339a327cc48d8a56358f512674975": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "65604f719f9e41f3b6d495d3aa907529": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "68759d18977d4757ba3eb3cf81aa1129": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "68dd48c190fe4b8c92790dc65803d785": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DropdownModel",
      "state": {
       "_options_labels": [
        "One",
        "Two",
        "Three"
       ],
       "description": "Number:",
       "index": 1,
       "layout": "IPY_MODEL_371339a327cc48d8a56358f512674975",
       "style": "IPY_MODEL_89e645fe88da40b2890bc1c08b50be78"
      }
     },
     "839595af423541ef946ce08e437e5b7e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "89e645fe88da40b2890bc1c08b50be78": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "9091258d01434bf8ac0f41c12363e6a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatTextModel",
      "state": {
       "layout": "IPY_MODEL_c22c18c67fe24f06ada7d28e3cb6ff90",
       "step": null,
       "style": "IPY_MODEL_839595af423541ef946ce08e437e5b7e"
      }
     },
     "afadc0b9a4f3458685319c1818955895": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DropdownModel",
      "state": {
       "_options_labels": [
        "One",
        "Two",
        "Three"
       ],
       "description": "Number:",
       "index": 1,
       "layout": "IPY_MODEL_1ee2806f2e314951b8b42666540f6aee",
       "style": "IPY_MODEL_093e89bffde74c5a8863a1d8ffe0bb66"
      }
     },
     "c22c18c67fe24f06ada7d28e3cb6ff90": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c64dd728486c4d4396b6b2b4a02ca054": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatSliderModel",
      "state": {
       "layout": "IPY_MODEL_1a5365fd770e464891fe02b94cb34b95",
       "step": 0.1,
       "style": "IPY_MODEL_ff2f617f26294276834c51733d0f253c"
      }
     },
     "d7c60be1aeb643fe9d80a622e61b4cc2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fe21ecb1be5e431c9745cf04421dc9a0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatSliderModel",
      "state": {
       "layout": "IPY_MODEL_d7c60be1aeb643fe9d80a622e61b4cc2",
       "step": 0.1,
       "style": "IPY_MODEL_65604f719f9e41f3b6d495d3aa907529",
       "value": 55.4
      }
     },
     "feea9000784940f3b601fd1ba613dd31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "ff2f617f26294276834c51733d0f253c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}