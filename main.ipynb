{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "<span style=\"font-family:Lucida Bright;\">\n",
    "<p style=\"margin-bottom:0.5cm\"></p>\n",
    "<center>\n",
    "<font size=\"8\"><b>Deep Learning, Fall 2021</b></font>\n",
    "<p style=\"margin-bottom:0.6cm\"></p>\n",
    "<font size=\"3\"><b>Final Project:</b></font>\n",
    "<p style=\"margin-bottom:0.6cm\"></p>\n",
    "<font size=\"5\"><b>Enhancing Voices for Better Speech Intelligibility</b></font>\n",
    "<p style=\"margin-bottom:2cm\"></p>\n",
    "<font size=\"6\"><b>Start</b></font>\n",
    "</center>\n",
    "<p style=\"margin-bottom:2cm\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Initialization\" data-toc-modified-id=\"Initialization-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Initialization</a></span></li><li><span><a href=\"#Load-data\" data-toc-modified-id=\"Load-data-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Load data</a></span></li></ul></li><li><span><a href=\"#Load-wav-file\" data-toc-modified-id=\"Load-wav-file-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Load wav file</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initialization procedure is defined in the notebook: [Initialization](intialization.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %run ./initialization.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from toolbox.initialization import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's define a function that will load the information from files representing a given sentence from a folder of our choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_sentence_data(sentence_code: str,\n",
    "                      path_folder: Path):\n",
    "    # Define the extensions of the files containing\n",
    "    # the data about the sentences.\n",
    "    file_extensions = ['wav', 'txt', 'wrd', 'phn']\n",
    "\n",
    "    # Create a path for each file type.\n",
    "    sentence_paths = {\n",
    "        extension: path_folder / f'{sentence_code}.{extension}'\n",
    "        for extension in file_extensions\n",
    "    }\n",
    "\n",
    "    ## Sentence data.\n",
    "    # Get the content of the 'txt' file as a list of lines.\n",
    "    file_content = (\n",
    "        sentence_paths['txt']\n",
    "            .read_text()\n",
    "            .split('\\n')\n",
    "    )\n",
    "\n",
    "    # Get the data about the sentence\n",
    "    sentence = dict()\n",
    "    sentence_data = file_content[0].strip().split()\n",
    "    sentence['start_sample'] = int(sentence_data[0])\n",
    "    sentence['end_sample'] = int(sentence_data[1])\n",
    "    sentence['text'] = ' '.join(sentence_data[2:])\n",
    "    sentence['audio_path'] = sentence_paths['wav']\n",
    "\n",
    "    ## Word data\n",
    "    # Get the content of the 'wrd' as a list of lines.\n",
    "    file_content = (\n",
    "        sentence_paths['wrd']\n",
    "            .read_text()\n",
    "            .split('\\n')\n",
    "    )\n",
    "\n",
    "    # Get the data about the words in the sentence.\n",
    "    words = list()\n",
    "    for line in file_content:\n",
    "        # Skip empty lines.\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        # Initialize the dict in which the data about the words\n",
    "        # will be saved.\n",
    "        word = dict()\n",
    "\n",
    "        # Extract the data about the words in the sentence.\n",
    "        word_data = line.strip().split()\n",
    "        word['start_sample'] = int(word_data[0])\n",
    "        word['end_sample'] = int(word_data[1])\n",
    "        word['text'] = ' '.join(word_data[2:])\n",
    "\n",
    "        # Append the extracted data to the list of words.\n",
    "        words.append(word)\n",
    "\n",
    "    ## Get the data about the phonemes in the sentence.\n",
    "    # Get the content of the 'wrd' as a list of lines.\n",
    "    file_content = (\n",
    "        sentence_paths['phn']\n",
    "            .read_text()\n",
    "            .split('\\n')\n",
    "    )\n",
    "\n",
    "    # Get the data about the words sentence.\n",
    "    phonemes = list()\n",
    "    for line in file_content:\n",
    "        # Skip empty lines.\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        # Initialize the dict in which the data about the words\n",
    "        # will be saved.\n",
    "        phoneme = dict()\n",
    "\n",
    "        # Extract the data about the words in the sentence.\n",
    "        phoneme_data = line.strip().split()\n",
    "        phoneme['start_sample'] = int(phoneme_data[0])\n",
    "        phoneme['end_sample'] = int(phoneme_data[1])\n",
    "        phoneme['text'] = ' '.join(phoneme_data[2:])\n",
    "\n",
    "        # Append the extracted data to the list of words.\n",
    "        phonemes.append(phoneme)\n",
    "\n",
    "    return sentence, words, phonemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, let's get the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# column_names = [\n",
    "#     'data_group',\n",
    "#     'dialect',\n",
    "#     'gender',\n",
    "#     'speaker',\n",
    "#     'type',\n",
    "#     'text',\n",
    "#     'audio_path',\n",
    "#     'start_sample',\n",
    "#     'end_sample',\n",
    "#     'words_text',\n",
    "#     'words_start_sample',\n",
    "#     'words_end_sample',\n",
    "#     'phonemes_text',\n",
    "#     'phonemes_start_sample',\n",
    "#     'phonemes_end_sample'\n",
    "# ]\n",
    "#\n",
    "# df_sentences = pd.DataFrame(columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Define the folder paths for the training and test data\n",
    "# data_folders = dict(\n",
    "#     train=paths.data.train,\n",
    "#     test=paths.data.test\n",
    "# )\n",
    "#\n",
    "# count = 0\n",
    "#\n",
    "# for data_group, path_data_folder in data_folders.items():\n",
    "#     # Get the dialects.\n",
    "#     dialects = [\n",
    "#         folder.stem\n",
    "#         for folder in path_data_folder.iterdir()\n",
    "#         if folder.is_dir()\n",
    "#     ]\n",
    "#\n",
    "#     # Get the speaker codes.\n",
    "#     for dialect in dialects:\n",
    "#         # Get the path to the folder representing\n",
    "#         # a given dialect.\n",
    "#         path_dialect_folder = path_data_folder / dialect\n",
    "#\n",
    "#         # Get the speaker codes present in the folder.\n",
    "#         speaker_codes = [\n",
    "#             folder.stem\n",
    "#             for folder in path_dialect_folder.iterdir()\n",
    "#             if folder.is_dir()\n",
    "#         ]\n",
    "#\n",
    "#         # Get information for each speaker.\n",
    "#         for speaker_code in speaker_codes:\n",
    "#             # Get the gender and ID code for the speaker.\n",
    "#             gender = speaker_code[0]\n",
    "#             speaker = speaker_code[1:]\n",
    "#\n",
    "#             # Define the path to the folder containing the\n",
    "#             # sentences spoken by the speaker.\n",
    "#             speaker_folder = path_dialect_folder / speaker_code\n",
    "#\n",
    "#             # Get the codes for all the sentences present in the\n",
    "#             # speaker folder.\n",
    "#             sentence_codes = [\n",
    "#                 path.stem\n",
    "#                 for path in speaker_folder.glob('**/*.wav')\n",
    "#             ]\n",
    "#\n",
    "#             # Get the information for each sentence.\n",
    "#             for sentence_code in sentence_codes:\n",
    "#                 # Extract and interpret the sentence types.\n",
    "#                 sentence_type_code = sentence_code[:2]\n",
    "#\n",
    "#                 if sentence_type_code.lower() == 'si':\n",
    "#                     sentence_type = 'phonetically-diverse'\n",
    "#\n",
    "#                 elif sentence_type_code.lower() == 'sa':\n",
    "#                     sentence_type = 'dialect'\n",
    "#\n",
    "#                 elif sentence_type_code.lower() == 'sx':\n",
    "#                     sentence_type = 'phonetically-compact'\n",
    "#\n",
    "#                 else:\n",
    "#                     raise ValueError(\n",
    "#                         f'Invalid sentence type: '\n",
    "#                         f'\"{sentence_type_code}\"'\n",
    "#                     )\n",
    "#\n",
    "#                 # Extract the sentence number.\n",
    "#                 sentence_number = sentence_code[-1]\n",
    "#\n",
    "#                 # Get the data about the\n",
    "#                 sentence, words, phonemes = \\\n",
    "#                     get_sentence_data(sentence_code, speaker_folder)\n",
    "#\n",
    "#                 # Put the data about the data in a dict\n",
    "#                 new_row = {\n",
    "#                     'data_group': data_group,\n",
    "#                     'dialect': dialect,\n",
    "#                     'gender': gender,\n",
    "#                     'speaker': speaker,\n",
    "#                     'type': sentence_type,\n",
    "#                     'text': sentence['text'],\n",
    "#                     'audio_path': sentence['audio_path'],\n",
    "#                     'start_sample': sentence['start_sample'],\n",
    "#                     'end_sample': sentence['end_sample'],\n",
    "#                     'words_text': [word['text']\n",
    "#                                    for word in words],\n",
    "#                     'words_start_sample': [word['start_sample']\n",
    "#                                            for word in words],\n",
    "#                     'words_end_sample': [word['end_sample']\n",
    "#                                            for word in words],\n",
    "#                     'phonemes_text': [phoneme['text']\n",
    "#                                       for phoneme in phonemes],\n",
    "#                     'phonemes_start_sample': [phoneme['start_sample']\n",
    "#                                            for phoneme in phonemes],\n",
    "#                     'phonemes_end_sample': [phoneme['end_sample']\n",
    "#                                            for phoneme in phonemes]\n",
    "#                 }\n",
    "#\n",
    "#                 # Append the data to the sentence dataframe.\n",
    "#                 df_sentences = df_sentences.append(new_row,\n",
    "#                                                    ignore_index=True)\n",
    "#\n",
    "# # Save the dataframe to pickle\n",
    "# df_sentences.to_pickle(paths.cache.sentence_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data from memory:\n",
    "df_sentences = pd.read_pickle(paths.cache.sentence_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_group</th>\n",
       "      <th>dialect</th>\n",
       "      <th>gender</th>\n",
       "      <th>speaker</th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>audio_path</th>\n",
       "      <th>start_sample</th>\n",
       "      <th>end_sample</th>\n",
       "      <th>words_text</th>\n",
       "      <th>words_start_sample</th>\n",
       "      <th>words_end_sample</th>\n",
       "      <th>phonemes_text</th>\n",
       "      <th>phonemes_start_sample</th>\n",
       "      <th>phonemes_end_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>train</td>\n",
       "      <td>DR2</td>\n",
       "      <td>F</td>\n",
       "      <td>MMH0</td>\n",
       "      <td>phonetically-compact</td>\n",
       "      <td>They often go out in the evening.</td>\n",
       "      <td>G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...</td>\n",
       "      <td>0</td>\n",
       "      <td>33792</td>\n",
       "      <td>[they, often, go, out, in, the, evening]</td>\n",
       "      <td>[2360, 4842, 10026, 12616, 15749, 17721, 19938]</td>\n",
       "      <td>[4842, 10026, 12616, 15749, 17721, 19258, 25152]</td>\n",
       "      <td>[h#, dh, ey, aa, f, ix, ng, gcl, g, ow, aw, dx...</td>\n",
       "      <td>[0, 2360, 2924, 4842, 6760, 8260, 9080, 10026,...</td>\n",
       "      <td>[2360, 2924, 4842, 6760, 8260, 9080, 10026, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4155</th>\n",
       "      <td>train</td>\n",
       "      <td>DR7</td>\n",
       "      <td>M</td>\n",
       "      <td>KLR0</td>\n",
       "      <td>phonetically-compact</td>\n",
       "      <td>The government sought authorization of his cit...</td>\n",
       "      <td>G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...</td>\n",
       "      <td>0</td>\n",
       "      <td>47104</td>\n",
       "      <td>[the, government, sought, authorization, of, h...</td>\n",
       "      <td>[9333, 9744, 15303, 19480, 31137, 32779, 35619]</td>\n",
       "      <td>[9744, 15303, 19480, 31137, 32779, 35619, 45500]</td>\n",
       "      <td>[h#, th, ax-h, gcl, g, ah, r, m, ix, tcl, s, a...</td>\n",
       "      <td>[0, 9333, 9577, 9744, 10790, 11100, 12437, 128...</td>\n",
       "      <td>[9333, 9577, 9744, 10790, 11100, 12437, 12828,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4794</th>\n",
       "      <td>test</td>\n",
       "      <td>DR2</td>\n",
       "      <td>F</td>\n",
       "      <td>RAM1</td>\n",
       "      <td>phonetically-diverse</td>\n",
       "      <td>Another memo for sightseers: bring your camera...</td>\n",
       "      <td>G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...</td>\n",
       "      <td>0</td>\n",
       "      <td>71373</td>\n",
       "      <td>[another, memo, for, sightseers, bring, your, ...</td>\n",
       "      <td>[2440, 9024, 14955, 17503, 32980, 36396, 38251...</td>\n",
       "      <td>[9024, 14955, 17503, 31010, 36396, 38251, 4572...</td>\n",
       "      <td>[h#, q, ax, n, ah, dh, er, m, eh, m, ow, f, er...</td>\n",
       "      <td>[0, 2440, 2713, 3216, 4971, 6760, 7520, 9024, ...</td>\n",
       "      <td>[2440, 2713, 3216, 4971, 6760, 7520, 9024, 105...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2379</th>\n",
       "      <td>train</td>\n",
       "      <td>DR4</td>\n",
       "      <td>M</td>\n",
       "      <td>LSH0</td>\n",
       "      <td>phonetically-compact</td>\n",
       "      <td>Last year's gas shortage caused steep price in...</td>\n",
       "      <td>G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...</td>\n",
       "      <td>0</td>\n",
       "      <td>54887</td>\n",
       "      <td>[last, year's, gas, shortage, caused, steep, p...</td>\n",
       "      <td>[2590, 8610, 12200, 16250, 24680, 32920, 36260...</td>\n",
       "      <td>[8610, 12200, 16250, 24680, 32920, 36260, 4196...</td>\n",
       "      <td>[h#, l, ae, s, tcl, ch, y, ih, axr, z, gcl, g,...</td>\n",
       "      <td>[0, 2590, 3467, 5932, 6934, 7728, 8610, 9082, ...</td>\n",
       "      <td>[2590, 3467, 5932, 6934, 7728, 8610, 9082, 100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6020</th>\n",
       "      <td>test</td>\n",
       "      <td>DR7</td>\n",
       "      <td>F</td>\n",
       "      <td>SXA0</td>\n",
       "      <td>dialect</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...</td>\n",
       "      <td>0</td>\n",
       "      <td>57960</td>\n",
       "      <td>[she, had, your, dark, suit, in, greasy, wash,...</td>\n",
       "      <td>[2200, 5200, 9107, 11240, 17440, 23143, 26223,...</td>\n",
       "      <td>[5200, 9566, 11240, 17440, 23143, 26223, 32920...</td>\n",
       "      <td>[h#, sh, iy, hv, ae, dcl, jh, er, dcl, d, aa, ...</td>\n",
       "      <td>[0, 2200, 3960, 5200, 6080, 8360, 9107, 9566, ...</td>\n",
       "      <td>[2200, 3960, 5200, 6080, 8360, 9107, 9566, 112...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>train</td>\n",
       "      <td>DR2</td>\n",
       "      <td>M</td>\n",
       "      <td>RLJ0</td>\n",
       "      <td>phonetically-compact</td>\n",
       "      <td>Those who are not purists use canned vegetable...</td>\n",
       "      <td>G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...</td>\n",
       "      <td>0</td>\n",
       "      <td>54989</td>\n",
       "      <td>[those, who, are, not, purists, use, canned, v...</td>\n",
       "      <td>[2170, 5529, 7018, 8280, 12600, 20160, 23394, ...</td>\n",
       "      <td>[5529, 7018, 8280, 12600, 20160, 23394, 29160,...</td>\n",
       "      <td>[h#, dh, ow, z, hv, uw, er, nx, aa, tcl, p, y,...</td>\n",
       "      <td>[0, 2170, 2440, 4400, 5529, 5863, 7018, 8280, ...</td>\n",
       "      <td>[2170, 2440, 4400, 5529, 5863, 7018, 8280, 884...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5430</th>\n",
       "      <td>test</td>\n",
       "      <td>DR4</td>\n",
       "      <td>M</td>\n",
       "      <td>DRM0</td>\n",
       "      <td>dialect</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...</td>\n",
       "      <td>0</td>\n",
       "      <td>48538</td>\n",
       "      <td>[she, had, your, dark, suit, in, greasy, wash,...</td>\n",
       "      <td>[2570, 6099, 9453, 11159, 16240, 21383, 22544,...</td>\n",
       "      <td>[6099, 10100, 11159, 16240, 21383, 22544, 2748...</td>\n",
       "      <td>[h#, sh, iy, hv, eh, dcl, jh, axr, dcl, d, aa,...</td>\n",
       "      <td>[0, 2570, 4803, 6099, 7352, 8992, 9453, 10100,...</td>\n",
       "      <td>[2570, 4803, 6099, 7352, 8992, 9453, 10100, 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>train</td>\n",
       "      <td>DR2</td>\n",
       "      <td>F</td>\n",
       "      <td>LMA0</td>\n",
       "      <td>phonetically-compact</td>\n",
       "      <td>They remained lifelong friends and companions.</td>\n",
       "      <td>G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...</td>\n",
       "      <td>0</td>\n",
       "      <td>36660</td>\n",
       "      <td>[they, remained, lifelong, friends, and, compa...</td>\n",
       "      <td>[2040, 3200, 8691, 16158, 21947, 22680]</td>\n",
       "      <td>[3200, 8691, 16158, 21947, 22680, 34339]</td>\n",
       "      <td>[h#, dh, ih, r, axr, m, ey, n, dcl, d, l, ay, ...</td>\n",
       "      <td>[0, 2040, 2520, 3200, 3592, 4762, 5994, 7737, ...</td>\n",
       "      <td>[2040, 2520, 3200, 3592, 4762, 5994, 7737, 811...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4817</th>\n",
       "      <td>test</td>\n",
       "      <td>DR2</td>\n",
       "      <td>M</td>\n",
       "      <td>ABW0</td>\n",
       "      <td>phonetically-compact</td>\n",
       "      <td>If people were more generous, there would be n...</td>\n",
       "      <td>G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...</td>\n",
       "      <td>0</td>\n",
       "      <td>49972</td>\n",
       "      <td>[if, people, were, more, generous, there, woul...</td>\n",
       "      <td>[2320, 3760, 9566, 11995, 15960, 25322, 26569,...</td>\n",
       "      <td>[3760, 9566, 11995, 15960, 24310, 26569, 29040...</td>\n",
       "      <td>[h#, ix, f, pcl, p, iy, pcl, p, el, w, axr, m,...</td>\n",
       "      <td>[0, 2320, 2800, 3760, 4880, 5500, 6600, 7700, ...</td>\n",
       "      <td>[2320, 2800, 3760, 4880, 5500, 6600, 7700, 796...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1680</th>\n",
       "      <td>train</td>\n",
       "      <td>DR3</td>\n",
       "      <td>M</td>\n",
       "      <td>MAR0</td>\n",
       "      <td>dialect</td>\n",
       "      <td>She had your dark suit in greasy wash water al...</td>\n",
       "      <td>G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...</td>\n",
       "      <td>0</td>\n",
       "      <td>64308</td>\n",
       "      <td>[she, had, your, dark, suit, in, greasy, wash,...</td>\n",
       "      <td>[2340, 5765, 10213, 12116, 17867, 23680, 27650...</td>\n",
       "      <td>[5765, 10213, 12116, 17867, 23680, 27650, 3429...</td>\n",
       "      <td>[h#, sh, iy, hv, ae, dcl, d, y, axr, dcl, d, a...</td>\n",
       "      <td>[0, 2340, 4423, 5765, 6360, 8731, 9680, 10213,...</td>\n",
       "      <td>[2340, 4423, 5765, 6360, 8731, 9680, 10213, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>train</td>\n",
       "      <td>DR4</td>\n",
       "      <td>F</td>\n",
       "      <td>JWB1</td>\n",
       "      <td>phonetically-diverse</td>\n",
       "      <td>To prepare mustard cream, blend mustard with e...</td>\n",
       "      <td>G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...</td>\n",
       "      <td>0</td>\n",
       "      <td>66663</td>\n",
       "      <td>[to, prepare, mustard, cream, blend, mustard, ...</td>\n",
       "      <td>[4570, 5800, 11112, 17150, 23200, 28033, 34168...</td>\n",
       "      <td>[5800, 11112, 17150, 23200, 28033, 34168, 3670...</td>\n",
       "      <td>[h#, t, ax-h, pcl, p, axr, pcl, p, eh, r, m, a...</td>\n",
       "      <td>[0, 4570, 5527, 5800, 6320, 6800, 7440, 8122, ...</td>\n",
       "      <td>[4570, 5527, 5800, 6320, 6800, 7440, 8122, 906...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2269</th>\n",
       "      <td>train</td>\n",
       "      <td>DR4</td>\n",
       "      <td>M</td>\n",
       "      <td>JMM0</td>\n",
       "      <td>phonetically-compact</td>\n",
       "      <td>Cement is measured in cubic yards.</td>\n",
       "      <td>G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...</td>\n",
       "      <td>0</td>\n",
       "      <td>36660</td>\n",
       "      <td>[cement, is, measured, in, cubic, yards]</td>\n",
       "      <td>[2201, 10360, 12223, 18520, 20170, 26760]</td>\n",
       "      <td>[10360, 12223, 18520, 20170, 26760, 33867]</td>\n",
       "      <td>[h#, s, ax, m, eh, n, tcl, t, ih, z, m, eh, zh...</td>\n",
       "      <td>[0, 2201, 4520, 4887, 6025, 8280, 8715, 9320, ...</td>\n",
       "      <td>[2201, 4520, 4887, 6025, 8280, 8715, 9320, 103...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>train</td>\n",
       "      <td>DR1</td>\n",
       "      <td>M</td>\n",
       "      <td>RWS0</td>\n",
       "      <td>phonetically-compact</td>\n",
       "      <td>How much allowance do you get?</td>\n",
       "      <td>G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...</td>\n",
       "      <td>0</td>\n",
       "      <td>22529</td>\n",
       "      <td>[how, much, allowance, do, you, get]</td>\n",
       "      <td>[2280, 3509, 6600, 13160, 15200, 16920]</td>\n",
       "      <td>[3509, 6600, 13160, 15200, 16920, 21500]</td>\n",
       "      <td>[h#, hh, ah, m, ah, tcl, ch, ax, l, aw, ax, n,...</td>\n",
       "      <td>[0, 2280, 2760, 3509, 3843, 5096, 5600, 6600, ...</td>\n",
       "      <td>[2280, 2760, 3509, 3843, 5096, 5600, 6600, 724...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4884</th>\n",
       "      <td>test</td>\n",
       "      <td>DR2</td>\n",
       "      <td>M</td>\n",
       "      <td>JAR0</td>\n",
       "      <td>phonetically-diverse</td>\n",
       "      <td>Poach the apples in this syrup for twelve minu...</td>\n",
       "      <td>G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...</td>\n",
       "      <td>0</td>\n",
       "      <td>79668</td>\n",
       "      <td>[poach, the, apples, in, this, syrup, for, twe...</td>\n",
       "      <td>[2230, 5597, 9984, 16920, 18720, 20980, 28908,...</td>\n",
       "      <td>[5597, 9984, 16920, 18720, 23750, 28908, 30680...</td>\n",
       "      <td>[h#, p, ow, tcl, sh, dh, iy, ae, pcl, p, el, z...</td>\n",
       "      <td>[0, 2230, 3200, 5096, 5597, 7520, 8510, 9984, ...</td>\n",
       "      <td>[2230, 3200, 5096, 5597, 7520, 8510, 9984, 126...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5652</th>\n",
       "      <td>test</td>\n",
       "      <td>DR5</td>\n",
       "      <td>F</td>\n",
       "      <td>MAH0</td>\n",
       "      <td>phonetically-diverse</td>\n",
       "      <td>A flame would use up air.</td>\n",
       "      <td>G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...</td>\n",
       "      <td>0</td>\n",
       "      <td>29696</td>\n",
       "      <td>[a, flame, would, use, up, air]</td>\n",
       "      <td>[2160, 3080, 10527, 12295, 17520, 21600]</td>\n",
       "      <td>[3080, 10527, 12295, 17520, 21600, 26680]</td>\n",
       "      <td>[h#, ax, f, l, ey, m, w, ix, dcl, y, ux, z, ah...</td>\n",
       "      <td>[0, 2160, 3080, 5510, 6308, 9103, 10527, 11195...</td>\n",
       "      <td>[2160, 3080, 5510, 6308, 9103, 10527, 11195, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>train</td>\n",
       "      <td>DR2</td>\n",
       "      <td>M</td>\n",
       "      <td>MAA0</td>\n",
       "      <td>phonetically-compact</td>\n",
       "      <td>Cottage cheese with chives is delicious.</td>\n",
       "      <td>G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...</td>\n",
       "      <td>0</td>\n",
       "      <td>45261</td>\n",
       "      <td>[cottage, cheese, with, chives, is, delicious]</td>\n",
       "      <td>[2170, 9120, 13810, 16440, 27627, 32470]</td>\n",
       "      <td>[9120, 13810, 16440, 27627, 32470, 42790]</td>\n",
       "      <td>[h#, k, aa, dx, ih, zh, tcl, ch, iy, z, w, ix,...</td>\n",
       "      <td>[0, 2170, 3800, 4840, 5240, 6760, 8149, 9120, ...</td>\n",
       "      <td>[2170, 3800, 4840, 5240, 6760, 8149, 9120, 104...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5753</th>\n",
       "      <td>test</td>\n",
       "      <td>DR5</td>\n",
       "      <td>M</td>\n",
       "      <td>DWA0</td>\n",
       "      <td>phonetically-diverse</td>\n",
       "      <td>Coverage of primary literature will follow.</td>\n",
       "      <td>G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...</td>\n",
       "      <td>0</td>\n",
       "      <td>35021</td>\n",
       "      <td>[coverage, of, primary, literature, will, follow]</td>\n",
       "      <td>[2135, 7861, 8817, 16334, 23812, 26151]</td>\n",
       "      <td>[7861, 8817, 16334, 23812, 26151, 32125]</td>\n",
       "      <td>[h#, k, ah, v, r, ix, dcl, jh, ax, v, pcl, p, ...</td>\n",
       "      <td>[0, 2135, 3033, 4258, 5008, 5681, 6527, 6867, ...</td>\n",
       "      <td>[2135, 3033, 4258, 5008, 5681, 6527, 6867, 786...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081</th>\n",
       "      <td>train</td>\n",
       "      <td>DR2</td>\n",
       "      <td>M</td>\n",
       "      <td>TAT1</td>\n",
       "      <td>dialect</td>\n",
       "      <td>Don't ask me to carry an oily rag like that.</td>\n",
       "      <td>G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...</td>\n",
       "      <td>0</td>\n",
       "      <td>46183</td>\n",
       "      <td>[don't, ask, me, to, carry, an, oily, rag, lik...</td>\n",
       "      <td>[2129, 5520, 11546, 13720, 15320, 21389, 22642...</td>\n",
       "      <td>[5520, 11546, 13720, 15320, 21389, 22642, 3084...</td>\n",
       "      <td>[h#, d, ow, n, q, ae, s, kcl, k, m, iy, dcl, d...</td>\n",
       "      <td>[0, 2129, 2494, 4684, 4840, 5520, 8850, 10120,...</td>\n",
       "      <td>[2129, 2494, 4684, 4840, 5520, 8850, 10120, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3545</th>\n",
       "      <td>train</td>\n",
       "      <td>DR6</td>\n",
       "      <td>M</td>\n",
       "      <td>RMB0</td>\n",
       "      <td>phonetically-compact</td>\n",
       "      <td>Pledge to participate in Nevada's aquatic comp...</td>\n",
       "      <td>G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...</td>\n",
       "      <td>0</td>\n",
       "      <td>53248</td>\n",
       "      <td>[pledge, to, participate, in, nevada's, aquati...</td>\n",
       "      <td>[2200, 8160, 11000, 23944, 26315, 34469, 41850]</td>\n",
       "      <td>[8160, 11000, 23944, 26315, 34469, 41850, 52213]</td>\n",
       "      <td>[h#, p, l, eh, dcl, jh, tcl, t, ux, pcl, p, ax...</td>\n",
       "      <td>[0, 2200, 2770, 4084, 6290, 6720, 8160, 8640, ...</td>\n",
       "      <td>[2200, 2770, 4084, 6290, 6720, 8160, 8640, 952...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5809</th>\n",
       "      <td>test</td>\n",
       "      <td>DR5</td>\n",
       "      <td>M</td>\n",
       "      <td>RJM3</td>\n",
       "      <td>phonetically-compact</td>\n",
       "      <td>Guess the question from the answer.</td>\n",
       "      <td>G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...</td>\n",
       "      <td>0</td>\n",
       "      <td>31335</td>\n",
       "      <td>[guess, the, question, from, the, answer]</td>\n",
       "      <td>[2330, 6440, 7960, 17079, 20720, 22516]</td>\n",
       "      <td>[6440, 7960, 17079, 20720, 22516, 28680]</td>\n",
       "      <td>[h#, g, eh, s, dh, ix, kcl, k, w, eh, sh, tcl,...</td>\n",
       "      <td>[0, 2330, 2790, 4680, 6440, 7430, 7960, 9150, ...</td>\n",
       "      <td>[2330, 2790, 4680, 6440, 7430, 7960, 9150, 100...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     data_group dialect gender speaker                  type  \\\n",
       "545       train     DR2      F    MMH0  phonetically-compact   \n",
       "4155      train     DR7      M    KLR0  phonetically-compact   \n",
       "4794       test     DR2      F    RAM1  phonetically-diverse   \n",
       "2379      train     DR4      M    LSH0  phonetically-compact   \n",
       "6020       test     DR7      F    SXA0               dialect   \n",
       "1048      train     DR2      M    RLJ0  phonetically-compact   \n",
       "5430       test     DR4      M    DRM0               dialect   \n",
       "505       train     DR2      F    LMA0  phonetically-compact   \n",
       "4817       test     DR2      M    ABW0  phonetically-compact   \n",
       "1680      train     DR3      M    MAR0               dialect   \n",
       "1963      train     DR4      F    JWB1  phonetically-diverse   \n",
       "2269      train     DR4      M    JMM0  phonetically-compact   \n",
       "326       train     DR1      M    RWS0  phonetically-compact   \n",
       "4884       test     DR2      M    JAR0  phonetically-diverse   \n",
       "5652       test     DR5      F    MAH0  phonetically-diverse   \n",
       "887       train     DR2      M    MAA0  phonetically-compact   \n",
       "5753       test     DR5      M    DWA0  phonetically-diverse   \n",
       "1081      train     DR2      M    TAT1               dialect   \n",
       "3545      train     DR6      M    RMB0  phonetically-compact   \n",
       "5809       test     DR5      M    RJM3  phonetically-compact   \n",
       "\n",
       "                                                   text  \\\n",
       "545                   They often go out in the evening.   \n",
       "4155  The government sought authorization of his cit...   \n",
       "4794  Another memo for sightseers: bring your camera...   \n",
       "2379  Last year's gas shortage caused steep price in...   \n",
       "6020  She had your dark suit in greasy wash water al...   \n",
       "1048  Those who are not purists use canned vegetable...   \n",
       "5430  She had your dark suit in greasy wash water al...   \n",
       "505      They remained lifelong friends and companions.   \n",
       "4817  If people were more generous, there would be n...   \n",
       "1680  She had your dark suit in greasy wash water al...   \n",
       "1963  To prepare mustard cream, blend mustard with e...   \n",
       "2269                 Cement is measured in cubic yards.   \n",
       "326                      How much allowance do you get?   \n",
       "4884  Poach the apples in this syrup for twelve minu...   \n",
       "5652                          A flame would use up air.   \n",
       "887            Cottage cheese with chives is delicious.   \n",
       "5753        Coverage of primary literature will follow.   \n",
       "1081       Don't ask me to carry an oily rag like that.   \n",
       "3545  Pledge to participate in Nevada's aquatic comp...   \n",
       "5809                Guess the question from the answer.   \n",
       "\n",
       "                                             audio_path start_sample  \\\n",
       "545   G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...            0   \n",
       "4155  G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...            0   \n",
       "4794  G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...            0   \n",
       "2379  G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...            0   \n",
       "6020  G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...            0   \n",
       "1048  G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...            0   \n",
       "5430  G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...            0   \n",
       "505   G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...            0   \n",
       "4817  G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...            0   \n",
       "1680  G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...            0   \n",
       "1963  G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...            0   \n",
       "2269  G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...            0   \n",
       "326   G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...            0   \n",
       "4884  G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...            0   \n",
       "5652  G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...            0   \n",
       "887   G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...            0   \n",
       "5753  G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...            0   \n",
       "1081  G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...            0   \n",
       "3545  G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...            0   \n",
       "5809  G:\\My Drive\\DTU\\Kurser\\Deep_Learning_02456\\fin...            0   \n",
       "\n",
       "     end_sample                                         words_text  \\\n",
       "545       33792           [they, often, go, out, in, the, evening]   \n",
       "4155      47104  [the, government, sought, authorization, of, h...   \n",
       "4794      71373  [another, memo, for, sightseers, bring, your, ...   \n",
       "2379      54887  [last, year's, gas, shortage, caused, steep, p...   \n",
       "6020      57960  [she, had, your, dark, suit, in, greasy, wash,...   \n",
       "1048      54989  [those, who, are, not, purists, use, canned, v...   \n",
       "5430      48538  [she, had, your, dark, suit, in, greasy, wash,...   \n",
       "505       36660  [they, remained, lifelong, friends, and, compa...   \n",
       "4817      49972  [if, people, were, more, generous, there, woul...   \n",
       "1680      64308  [she, had, your, dark, suit, in, greasy, wash,...   \n",
       "1963      66663  [to, prepare, mustard, cream, blend, mustard, ...   \n",
       "2269      36660           [cement, is, measured, in, cubic, yards]   \n",
       "326       22529               [how, much, allowance, do, you, get]   \n",
       "4884      79668  [poach, the, apples, in, this, syrup, for, twe...   \n",
       "5652      29696                    [a, flame, would, use, up, air]   \n",
       "887       45261     [cottage, cheese, with, chives, is, delicious]   \n",
       "5753      35021  [coverage, of, primary, literature, will, follow]   \n",
       "1081      46183  [don't, ask, me, to, carry, an, oily, rag, lik...   \n",
       "3545      53248  [pledge, to, participate, in, nevada's, aquati...   \n",
       "5809      31335          [guess, the, question, from, the, answer]   \n",
       "\n",
       "                                     words_start_sample  \\\n",
       "545     [2360, 4842, 10026, 12616, 15749, 17721, 19938]   \n",
       "4155    [9333, 9744, 15303, 19480, 31137, 32779, 35619]   \n",
       "4794  [2440, 9024, 14955, 17503, 32980, 36396, 38251...   \n",
       "2379  [2590, 8610, 12200, 16250, 24680, 32920, 36260...   \n",
       "6020  [2200, 5200, 9107, 11240, 17440, 23143, 26223,...   \n",
       "1048  [2170, 5529, 7018, 8280, 12600, 20160, 23394, ...   \n",
       "5430  [2570, 6099, 9453, 11159, 16240, 21383, 22544,...   \n",
       "505             [2040, 3200, 8691, 16158, 21947, 22680]   \n",
       "4817  [2320, 3760, 9566, 11995, 15960, 25322, 26569,...   \n",
       "1680  [2340, 5765, 10213, 12116, 17867, 23680, 27650...   \n",
       "1963  [4570, 5800, 11112, 17150, 23200, 28033, 34168...   \n",
       "2269          [2201, 10360, 12223, 18520, 20170, 26760]   \n",
       "326             [2280, 3509, 6600, 13160, 15200, 16920]   \n",
       "4884  [2230, 5597, 9984, 16920, 18720, 20980, 28908,...   \n",
       "5652           [2160, 3080, 10527, 12295, 17520, 21600]   \n",
       "887            [2170, 9120, 13810, 16440, 27627, 32470]   \n",
       "5753            [2135, 7861, 8817, 16334, 23812, 26151]   \n",
       "1081  [2129, 5520, 11546, 13720, 15320, 21389, 22642...   \n",
       "3545    [2200, 8160, 11000, 23944, 26315, 34469, 41850]   \n",
       "5809            [2330, 6440, 7960, 17079, 20720, 22516]   \n",
       "\n",
       "                                       words_end_sample  \\\n",
       "545    [4842, 10026, 12616, 15749, 17721, 19258, 25152]   \n",
       "4155   [9744, 15303, 19480, 31137, 32779, 35619, 45500]   \n",
       "4794  [9024, 14955, 17503, 31010, 36396, 38251, 4572...   \n",
       "2379  [8610, 12200, 16250, 24680, 32920, 36260, 4196...   \n",
       "6020  [5200, 9566, 11240, 17440, 23143, 26223, 32920...   \n",
       "1048  [5529, 7018, 8280, 12600, 20160, 23394, 29160,...   \n",
       "5430  [6099, 10100, 11159, 16240, 21383, 22544, 2748...   \n",
       "505            [3200, 8691, 16158, 21947, 22680, 34339]   \n",
       "4817  [3760, 9566, 11995, 15960, 24310, 26569, 29040...   \n",
       "1680  [5765, 10213, 12116, 17867, 23680, 27650, 3429...   \n",
       "1963  [5800, 11112, 17150, 23200, 28033, 34168, 3670...   \n",
       "2269         [10360, 12223, 18520, 20170, 26760, 33867]   \n",
       "326            [3509, 6600, 13160, 15200, 16920, 21500]   \n",
       "4884  [5597, 9984, 16920, 18720, 23750, 28908, 30680...   \n",
       "5652          [3080, 10527, 12295, 17520, 21600, 26680]   \n",
       "887           [9120, 13810, 16440, 27627, 32470, 42790]   \n",
       "5753           [7861, 8817, 16334, 23812, 26151, 32125]   \n",
       "1081  [5520, 11546, 13720, 15320, 21389, 22642, 3084...   \n",
       "3545   [8160, 11000, 23944, 26315, 34469, 41850, 52213]   \n",
       "5809           [6440, 7960, 17079, 20720, 22516, 28680]   \n",
       "\n",
       "                                          phonemes_text  \\\n",
       "545   [h#, dh, ey, aa, f, ix, ng, gcl, g, ow, aw, dx...   \n",
       "4155  [h#, th, ax-h, gcl, g, ah, r, m, ix, tcl, s, a...   \n",
       "4794  [h#, q, ax, n, ah, dh, er, m, eh, m, ow, f, er...   \n",
       "2379  [h#, l, ae, s, tcl, ch, y, ih, axr, z, gcl, g,...   \n",
       "6020  [h#, sh, iy, hv, ae, dcl, jh, er, dcl, d, aa, ...   \n",
       "1048  [h#, dh, ow, z, hv, uw, er, nx, aa, tcl, p, y,...   \n",
       "5430  [h#, sh, iy, hv, eh, dcl, jh, axr, dcl, d, aa,...   \n",
       "505   [h#, dh, ih, r, axr, m, ey, n, dcl, d, l, ay, ...   \n",
       "4817  [h#, ix, f, pcl, p, iy, pcl, p, el, w, axr, m,...   \n",
       "1680  [h#, sh, iy, hv, ae, dcl, d, y, axr, dcl, d, a...   \n",
       "1963  [h#, t, ax-h, pcl, p, axr, pcl, p, eh, r, m, a...   \n",
       "2269  [h#, s, ax, m, eh, n, tcl, t, ih, z, m, eh, zh...   \n",
       "326   [h#, hh, ah, m, ah, tcl, ch, ax, l, aw, ax, n,...   \n",
       "4884  [h#, p, ow, tcl, sh, dh, iy, ae, pcl, p, el, z...   \n",
       "5652  [h#, ax, f, l, ey, m, w, ix, dcl, y, ux, z, ah...   \n",
       "887   [h#, k, aa, dx, ih, zh, tcl, ch, iy, z, w, ix,...   \n",
       "5753  [h#, k, ah, v, r, ix, dcl, jh, ax, v, pcl, p, ...   \n",
       "1081  [h#, d, ow, n, q, ae, s, kcl, k, m, iy, dcl, d...   \n",
       "3545  [h#, p, l, eh, dcl, jh, tcl, t, ux, pcl, p, ax...   \n",
       "5809  [h#, g, eh, s, dh, ix, kcl, k, w, eh, sh, tcl,...   \n",
       "\n",
       "                                  phonemes_start_sample  \\\n",
       "545   [0, 2360, 2924, 4842, 6760, 8260, 9080, 10026,...   \n",
       "4155  [0, 9333, 9577, 9744, 10790, 11100, 12437, 128...   \n",
       "4794  [0, 2440, 2713, 3216, 4971, 6760, 7520, 9024, ...   \n",
       "2379  [0, 2590, 3467, 5932, 6934, 7728, 8610, 9082, ...   \n",
       "6020  [0, 2200, 3960, 5200, 6080, 8360, 9107, 9566, ...   \n",
       "1048  [0, 2170, 2440, 4400, 5529, 5863, 7018, 8280, ...   \n",
       "5430  [0, 2570, 4803, 6099, 7352, 8992, 9453, 10100,...   \n",
       "505   [0, 2040, 2520, 3200, 3592, 4762, 5994, 7737, ...   \n",
       "4817  [0, 2320, 2800, 3760, 4880, 5500, 6600, 7700, ...   \n",
       "1680  [0, 2340, 4423, 5765, 6360, 8731, 9680, 10213,...   \n",
       "1963  [0, 4570, 5527, 5800, 6320, 6800, 7440, 8122, ...   \n",
       "2269  [0, 2201, 4520, 4887, 6025, 8280, 8715, 9320, ...   \n",
       "326   [0, 2280, 2760, 3509, 3843, 5096, 5600, 6600, ...   \n",
       "4884  [0, 2230, 3200, 5096, 5597, 7520, 8510, 9984, ...   \n",
       "5652  [0, 2160, 3080, 5510, 6308, 9103, 10527, 11195...   \n",
       "887   [0, 2170, 3800, 4840, 5240, 6760, 8149, 9120, ...   \n",
       "5753  [0, 2135, 3033, 4258, 5008, 5681, 6527, 6867, ...   \n",
       "1081  [0, 2129, 2494, 4684, 4840, 5520, 8850, 10120,...   \n",
       "3545  [0, 2200, 2770, 4084, 6290, 6720, 8160, 8640, ...   \n",
       "5809  [0, 2330, 2790, 4680, 6440, 7430, 7960, 9150, ...   \n",
       "\n",
       "                                    phonemes_end_sample  \n",
       "545   [2360, 2924, 4842, 6760, 8260, 9080, 10026, 10...  \n",
       "4155  [9333, 9577, 9744, 10790, 11100, 12437, 12828,...  \n",
       "4794  [2440, 2713, 3216, 4971, 6760, 7520, 9024, 105...  \n",
       "2379  [2590, 3467, 5932, 6934, 7728, 8610, 9082, 100...  \n",
       "6020  [2200, 3960, 5200, 6080, 8360, 9107, 9566, 112...  \n",
       "1048  [2170, 2440, 4400, 5529, 5863, 7018, 8280, 884...  \n",
       "5430  [2570, 4803, 6099, 7352, 8992, 9453, 10100, 11...  \n",
       "505   [2040, 2520, 3200, 3592, 4762, 5994, 7737, 811...  \n",
       "4817  [2320, 2800, 3760, 4880, 5500, 6600, 7700, 796...  \n",
       "1680  [2340, 4423, 5765, 6360, 8731, 9680, 10213, 10...  \n",
       "1963  [4570, 5527, 5800, 6320, 6800, 7440, 8122, 906...  \n",
       "2269  [2201, 4520, 4887, 6025, 8280, 8715, 9320, 103...  \n",
       "326   [2280, 2760, 3509, 3843, 5096, 5600, 6600, 724...  \n",
       "4884  [2230, 3200, 5096, 5597, 7520, 8510, 9984, 126...  \n",
       "5652  [2160, 3080, 5510, 6308, 9103, 10527, 11195, 1...  \n",
       "887   [2170, 3800, 4840, 5240, 6760, 8149, 9120, 104...  \n",
       "5753  [2135, 3033, 4258, 5008, 5681, 6527, 6867, 786...  \n",
       "1081  [2129, 2494, 4684, 4840, 5520, 8850, 10120, 10...  \n",
       "3545  [2200, 2770, 4084, 6290, 6720, 8160, 8640, 952...  \n",
       "5809  [2330, 2790, 4680, 6440, 7430, 7960, 9150, 100...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_sentences.sample(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load wav file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:.conda-DataScience] *",
   "language": "python",
   "name": "conda-env-.conda-DataScience-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "445px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "093e89bffde74c5a8863a1d8ffe0bb66": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "1a5365fd770e464891fe02b94cb34b95": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1c7a498865af41a1bd4af63eca07bd28": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatTextModel",
      "state": {
       "layout": "IPY_MODEL_68759d18977d4757ba3eb3cf81aa1129",
       "step": null,
       "style": "IPY_MODEL_feea9000784940f3b601fd1ba613dd31"
      }
     },
     "1ee2806f2e314951b8b42666540f6aee": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "371339a327cc48d8a56358f512674975": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "65604f719f9e41f3b6d495d3aa907529": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "68759d18977d4757ba3eb3cf81aa1129": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "68dd48c190fe4b8c92790dc65803d785": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DropdownModel",
      "state": {
       "_options_labels": [
        "One",
        "Two",
        "Three"
       ],
       "description": "Number:",
       "index": 1,
       "layout": "IPY_MODEL_371339a327cc48d8a56358f512674975",
       "style": "IPY_MODEL_89e645fe88da40b2890bc1c08b50be78"
      }
     },
     "839595af423541ef946ce08e437e5b7e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "89e645fe88da40b2890bc1c08b50be78": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "9091258d01434bf8ac0f41c12363e6a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatTextModel",
      "state": {
       "layout": "IPY_MODEL_c22c18c67fe24f06ada7d28e3cb6ff90",
       "step": null,
       "style": "IPY_MODEL_839595af423541ef946ce08e437e5b7e"
      }
     },
     "afadc0b9a4f3458685319c1818955895": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DropdownModel",
      "state": {
       "_options_labels": [
        "One",
        "Two",
        "Three"
       ],
       "description": "Number:",
       "index": 1,
       "layout": "IPY_MODEL_1ee2806f2e314951b8b42666540f6aee",
       "style": "IPY_MODEL_093e89bffde74c5a8863a1d8ffe0bb66"
      }
     },
     "c22c18c67fe24f06ada7d28e3cb6ff90": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c64dd728486c4d4396b6b2b4a02ca054": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatSliderModel",
      "state": {
       "layout": "IPY_MODEL_1a5365fd770e464891fe02b94cb34b95",
       "step": 0.1,
       "style": "IPY_MODEL_ff2f617f26294276834c51733d0f253c"
      }
     },
     "d7c60be1aeb643fe9d80a622e61b4cc2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fe21ecb1be5e431c9745cf04421dc9a0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatSliderModel",
      "state": {
       "layout": "IPY_MODEL_d7c60be1aeb643fe9d80a622e61b4cc2",
       "step": 0.1,
       "style": "IPY_MODEL_65604f719f9e41f3b6d495d3aa907529",
       "value": 55.4
      }
     },
     "feea9000784940f3b601fd1ba613dd31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "ff2f617f26294276834c51733d0f253c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}