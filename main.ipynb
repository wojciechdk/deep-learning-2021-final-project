{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "<span style=\"font-family:Lucida Bright;\">\n",
    "<p style=\"margin-bottom:0.5cm\"></p>\n",
    "<center>\n",
    "<font size=\"8\"><b>Deep Learning, Fall 2021</b></font>\n",
    "<p style=\"margin-bottom:0.6cm\"></p>\n",
    "<font size=\"3\"><b>Final Project:</b></font>\n",
    "<p style=\"margin-bottom:0.6cm\"></p>\n",
    "<font size=\"5\"><b>Enhancing Voices for Better Speech Intelligibility</b></font>\n",
    "<p style=\"margin-bottom:2cm\"></p>\n",
    "<font size=\"6\"><b>Start</b></font>\n",
    "</center>\n",
    "<p style=\"margin-bottom:2cm\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#How-to-get-the-most-out-of-this-notebook\" data-toc-modified-id=\"How-to-get-the-most-out-of-this-notebook-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>How to get the most out of this notebook</a></span></li><li><span><a href=\"#Prerequisites\" data-toc-modified-id=\"Prerequisites-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Prerequisites</a></span></li><li><span><a href=\"#Links-and-resources\" data-toc-modified-id=\"Links-and-resources-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Links and resources</a></span></li></ul></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Initialization\" data-toc-modified-id=\"Initialization-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Initialization</a></span></li><li><span><a href=\"#Load-data\" data-toc-modified-id=\"Load-data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Load data</a></span><ul class=\"toc-item\"><li><span><a href=\"#TIMIT-dataset\" data-toc-modified-id=\"TIMIT-dataset-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>TIMIT dataset</a></span></li><li><span><a href=\"#Synthetic-speech-dataset\" data-toc-modified-id=\"Synthetic-speech-dataset-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Synthetic speech dataset</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Many people struggle to understand speech in challenging acoustic environments, such as noisy bar. Therefore, enhancing the intelligibility of noisy speech signals is one of the key challenges for any producer of modern communication devices.\n",
    "\n",
    "The problem is often tackled by dividing a noisy speech signal into a number of frequency bands and attenuating the ones where the signal-to-noise ratio is insufficient. This approach, while effective in some situations, often leads to poor results, and sometimes even exacerbates the problem it is trying to solve as the constant activation and disactivation of some of the frequency bands in response to the fluctuations in speech and noise can create a very unnatural and disturbing sounds.\n",
    "\n",
    "In this project, we will try a different approach and attempt to create a deep learning model that will produce an equalization curve that can be applied to the noisy speech signal in order to maximize its intelligibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Official project description"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the info-doc for the Deep Learning course, the project is described as follows:\n",
    "\n",
    "> *__Designing self-driving earbuds with [augmentedhearing.io](augmentedhearing.io) which enhance voices based on function correlated with speech intelligibility__*\n",
    ">\n",
    "> *As one in four adults struggle to understand speech in challenging acoustics we aim to train consumer earbuds to enhance voices through back propagation using DHASP model implemented using [PyTorch differentiation package](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9414571)* validated on [TIMIT speech dataset](https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3) based on an objective function correlated with [HASPI speech intelligibility auditory processing model](https://www.sciencedirect.com/science/article/pii/S0167639320300431) available in Matlab.*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The approach"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The time scope for the course is 7 days x 9 hours / day = 63 hours. The outcome should be documented in a report formatted as [conference paper](https://drive.google.com/file/d/0BxJRy96AHCJxaUEwOFhwUExmX00/view?usp=sharing&resourcekey=0-RvwJqDVrZVijbkkifLWoYA), as well as a Jupyter notebook that ideally should recreate the main results of the report.\n",
    "\n",
    "At the beginning of the project, the following was available:\n",
    "\n",
    "1. speech data: a TIMIT dataset containing of 10 sentences spoken by 630 speakers from 8 major dialect regions of the United States.\n",
    "2. an [article](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9414571) describing a proposal for a differentiable objective function that can be used to train a neural network. The function is a measure of similarity of a [cepstrum](https://en.wikipedia.org/wiki/Cepstrum) of a given speech signal to the cepstrum of the target signal (usually clear, noisless speech)\n",
    "3. Matlab code for calculating the [HASPI speech intelligibility index] that can be used to evaluate the results.\n",
    "\n",
    "To create a our model, the following is needed:\n",
    "\n",
    "1. clear speech audio data to use as the target for model\n",
    "2. the corresponding noisy speech audio data to train the mode\n",
    "3. a PyTorch implementation of the equalization filter that can be applied to the speech signal. Our model will optimize the parameters of this filter to maximize speech intelligibility .\n",
    "4. a working PyTorch implementation of the objective function proposed in the [article](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9414571).\n",
    "\n",
    "Therefore, before we even create our neural network, we need to obtain the prerequisites 2 - 4, out of which especially number 4: implementation of the objective function proposed in the [article](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9414571) is, as I will demonstrate later, all but straightforward. To go through and understand the project literature, obtain the prerequisites, create and optimize model, and document the findings is a task that extends way beyond the 63-hour scope of this course. The work presented in this notebook is a result more then 3 times as much mork, and contains a functioning implementation of the all the prerequisites necessary to build a neural network that will attempt to optimize our objective.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## How to get the most out of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fully enjoy the content of this notebook, please take note of the following:\n",
    "\n",
    "- The outputs of pre-executed cells will not be rendered properly unless the notebook is **Trusted**.\n",
    "\n",
    "- To avoid accidental changes, most of the cells in this notebook are marked-as read only, and many are frozen (i.e. disabled from being run). To take advantage of these features, it is recommended to use the extension\n",
    "[Freeze Cell](https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions/freeze/readme.html)  which works with Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a part of project. To be able to execute all of its content:\n",
    "\n",
    "1. Download the project repository: `https://github.com/wojciechdk/deep-learning-2021-final-project.git` and run the notebook from the root.\n",
    "\n",
    "2. Download the [TIMIT dataset](https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3), unpack it, and place it so that the folders *DOC*, *TEST*, and *TRAIN* are placed in the folder `[root]\\resources\\data\\TIMIT`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Links and resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you are reading the HTML version of this notebook and would like to download the ipynb source, you can do it [here](https://github.com/wojciechdk/deep-learning-2021-final-project/blob/main/main.ipynb).\n",
    "\n",
    "- The Git repository containing the entire project can be accessed [here](https://github.com/wojciechdk/deep-learning-2021-final-project.git)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The entire initialization process, including:\n",
    " - imports of the necessary packages\n",
    " - configuration of the notebook and packages\n",
    " - imports of the toolbox functions\n",
    "\n",
    "is defined in the file [toolbox/initialization.py](toolbox/initialization.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from toolbox.initialization import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### TIMIT dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The metadata about the TIMIT dataset is loaded using the function `load_timit_data` defined in the module `toolbox\\data_loading.py`. Once loaded, the data is saved in the project cache as a Pandas dataframe. Let's load it and show a couple of rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     sentence_number data_group        dialect gender speaker  \\\n3723            1681      train        Western      F    LEH0   \n1968             435      train  South Midland      F    JWB1   \n2088             439      train  South Midland      M    BWP0   \n4345             164      train        Western      M    TMN0   \n4524             937      train      Army Brat      M    KDD0   \n\n                      type                                               text  \\\n3723  phonetically-diverse            The sheriff's swivel chair tilted back.   \n1968  phonetically-compact  The rich should invest in black zircons instea...   \n2088  phonetically-compact  That stinging vapor was caused by chloride vap...   \n4345  phonetically-compact           Curiosity and mediocrity seldom coexist.   \n4524  phonetically-diverse  Let the orthodontist decide the proper time to...   \n\n                      audio_path start_sample end_sample  \\\n3723  train\\DR7\\FLEH0\\SI1681.wav            0      38503   \n1968   train\\DR4\\FJWB1\\SX435.wav            0      54375   \n2088   train\\DR4\\MBWP0\\SX439.wav            0      54989   \n4345   train\\DR7\\MTMN0\\SX164.wav            0      56730   \n4524   train\\DR8\\MKDD0\\SI937.wav            0      66253   \n\n                                             words_text  \\\n3723      [the, sheriff's, swivel, chair, tilted, back]   \n1968  [the, rich, should, invest, in, black, zircons...   \n2088  [that, stinging, vapor, was, caused, by, chlor...   \n4345      [curiosity, and, mediocrity, seldom, coexist]   \n4524  [let, the, orthodontist, decide, the, proper, ...   \n\n                                     words_start_sample  \\\n3723           [2308, 3360, 13066, 16793, 22057, 29490]   \n1968  [2140, 3160, 6340, 9942, 16420, 17754, 22050, ...   \n2088  [2230, 5390, 13280, 19258, 23101, 30258, 32360...   \n4345                 [3210, 17002, 19091, 33073, 40098]   \n4524  [1532, 6330, 8123, 20845, 28350, 29240, 35080,...   \n\n                                       words_end_sample  \\\n3723          [3360, 12600, 16793, 22057, 29490, 35550]   \n1968  [3160, 6340, 9942, 16420, 17754, 22050, 30590,...   \n2088  [5390, 13280, 19258, 23101, 30258, 32360, 4084...   \n4345                [17002, 19091, 33073, 40098, 53255]   \n4524  [6330, 8123, 20845, 28350, 29240, 35080, 40674...   \n\n                                          phonemes_text  \\\n3723  [h#, dh, ix, sh, eh, r, ix, f, s, epi, w, ih, ...   \n1968  [h#, dh, ax, r, ih, tcl, sh, uh, dx, uh, n, v,...   \n2088  [h#, dh, ae, tcl, s, tcl, t, ih, ng, iy, ng, v...   \n4345  [h#, k, y, er, iy, aa, s, ix, dx, iy, ih, n, m...   \n4524  [h#, l, eh, tcl, dh, iy, ao, r, th, ix, dcl, d...   \n\n                                  phonemes_start_sample  \\\n3723  [0, 2308, 2533, 3360, 6040, 7644, 8605, 9242, ...   \n1968  [0, 2140, 2260, 3160, 4386, 5890, 6340, 8760, ...   \n2088  [0, 2230, 2715, 4440, 5390, 6983, 7780, 8149, ...   \n4345  [0, 3210, 4660, 5389, 7352, 8689, 10986, 12814...   \n4524  [0, 1532, 3020, 4680, 6330, 6710, 8123, 9274, ...   \n\n                                    phonemes_end_sample  \n3723  [2308, 2533, 3360, 6040, 7644, 8605, 9242, 100...  \n1968  [2140, 2260, 3160, 4386, 5890, 6340, 8760, 973...  \n2088  [2230, 2715, 4440, 5390, 6983, 7780, 8149, 939...  \n4345  [3210, 4660, 5389, 7352, 8689, 10986, 12814, 1...  \n4524  [1532, 3020, 4680, 6330, 6710, 8123, 9274, 101...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence_number</th>\n      <th>data_group</th>\n      <th>dialect</th>\n      <th>gender</th>\n      <th>speaker</th>\n      <th>type</th>\n      <th>text</th>\n      <th>audio_path</th>\n      <th>start_sample</th>\n      <th>end_sample</th>\n      <th>words_text</th>\n      <th>words_start_sample</th>\n      <th>words_end_sample</th>\n      <th>phonemes_text</th>\n      <th>phonemes_start_sample</th>\n      <th>phonemes_end_sample</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3723</th>\n      <td>1681</td>\n      <td>train</td>\n      <td>Western</td>\n      <td>F</td>\n      <td>LEH0</td>\n      <td>phonetically-diverse</td>\n      <td>The sheriff's swivel chair tilted back.</td>\n      <td>train\\DR7\\FLEH0\\SI1681.wav</td>\n      <td>0</td>\n      <td>38503</td>\n      <td>[the, sheriff's, swivel, chair, tilted, back]</td>\n      <td>[2308, 3360, 13066, 16793, 22057, 29490]</td>\n      <td>[3360, 12600, 16793, 22057, 29490, 35550]</td>\n      <td>[h#, dh, ix, sh, eh, r, ix, f, s, epi, w, ih, ...</td>\n      <td>[0, 2308, 2533, 3360, 6040, 7644, 8605, 9242, ...</td>\n      <td>[2308, 2533, 3360, 6040, 7644, 8605, 9242, 100...</td>\n    </tr>\n    <tr>\n      <th>1968</th>\n      <td>435</td>\n      <td>train</td>\n      <td>South Midland</td>\n      <td>F</td>\n      <td>JWB1</td>\n      <td>phonetically-compact</td>\n      <td>The rich should invest in black zircons instea...</td>\n      <td>train\\DR4\\FJWB1\\SX435.wav</td>\n      <td>0</td>\n      <td>54375</td>\n      <td>[the, rich, should, invest, in, black, zircons...</td>\n      <td>[2140, 3160, 6340, 9942, 16420, 17754, 22050, ...</td>\n      <td>[3160, 6340, 9942, 16420, 17754, 22050, 30590,...</td>\n      <td>[h#, dh, ax, r, ih, tcl, sh, uh, dx, uh, n, v,...</td>\n      <td>[0, 2140, 2260, 3160, 4386, 5890, 6340, 8760, ...</td>\n      <td>[2140, 2260, 3160, 4386, 5890, 6340, 8760, 973...</td>\n    </tr>\n    <tr>\n      <th>2088</th>\n      <td>439</td>\n      <td>train</td>\n      <td>South Midland</td>\n      <td>M</td>\n      <td>BWP0</td>\n      <td>phonetically-compact</td>\n      <td>That stinging vapor was caused by chloride vap...</td>\n      <td>train\\DR4\\MBWP0\\SX439.wav</td>\n      <td>0</td>\n      <td>54989</td>\n      <td>[that, stinging, vapor, was, caused, by, chlor...</td>\n      <td>[2230, 5390, 13280, 19258, 23101, 30258, 32360...</td>\n      <td>[5390, 13280, 19258, 23101, 30258, 32360, 4084...</td>\n      <td>[h#, dh, ae, tcl, s, tcl, t, ih, ng, iy, ng, v...</td>\n      <td>[0, 2230, 2715, 4440, 5390, 6983, 7780, 8149, ...</td>\n      <td>[2230, 2715, 4440, 5390, 6983, 7780, 8149, 939...</td>\n    </tr>\n    <tr>\n      <th>4345</th>\n      <td>164</td>\n      <td>train</td>\n      <td>Western</td>\n      <td>M</td>\n      <td>TMN0</td>\n      <td>phonetically-compact</td>\n      <td>Curiosity and mediocrity seldom coexist.</td>\n      <td>train\\DR7\\MTMN0\\SX164.wav</td>\n      <td>0</td>\n      <td>56730</td>\n      <td>[curiosity, and, mediocrity, seldom, coexist]</td>\n      <td>[3210, 17002, 19091, 33073, 40098]</td>\n      <td>[17002, 19091, 33073, 40098, 53255]</td>\n      <td>[h#, k, y, er, iy, aa, s, ix, dx, iy, ih, n, m...</td>\n      <td>[0, 3210, 4660, 5389, 7352, 8689, 10986, 12814...</td>\n      <td>[3210, 4660, 5389, 7352, 8689, 10986, 12814, 1...</td>\n    </tr>\n    <tr>\n      <th>4524</th>\n      <td>937</td>\n      <td>train</td>\n      <td>Army Brat</td>\n      <td>M</td>\n      <td>KDD0</td>\n      <td>phonetically-diverse</td>\n      <td>Let the orthodontist decide the proper time to...</td>\n      <td>train\\DR8\\MKDD0\\SI937.wav</td>\n      <td>0</td>\n      <td>66253</td>\n      <td>[let, the, orthodontist, decide, the, proper, ...</td>\n      <td>[1532, 6330, 8123, 20845, 28350, 29240, 35080,...</td>\n      <td>[6330, 8123, 20845, 28350, 29240, 35080, 40674...</td>\n      <td>[h#, l, eh, tcl, dh, iy, ao, r, th, ix, dcl, d...</td>\n      <td>[0, 1532, 3020, 4680, 6330, 6710, 8123, 9274, ...</td>\n      <td>[1532, 3020, 4680, 6330, 6710, 8123, 9274, 101...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the TIMIT meta data from the cache.\n",
    "df_timit = pd.read_pickle(paths.cache.df_timit)\n",
    "\n",
    "# Show the top 5 rows\n",
    "display(df_timit.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, let's play one sentence from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t.sound.play_timit(df_timit.loc[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Synthetic speech dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The metadata about the dataset containing synthetic speech in different audio settings is loaded using the function `load_synthetic_speech_data` defined in the module `toolbox\\data_loading.py`. Once loaded, the data is saved in the project cache as a Pandas dataframe. Let's load it and show a couple of rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      speaker length         variant segment fullness clarity  \\\n2174    kevin     5s          babble       3      NaN     NaN   \n1797   kendra     5s          babble       4      NaN     NaN   \n3051  matthew     5s              tv       7      NaN     NaN   \n1446   justin     5s           music      17      NaN     NaN   \n3223    salli     5s  zoom_augmented      33        1       4   \n\n                      audio_path  \n2174    cut_5_s\\kevin\\babb\\3.wav  \n1797   cut_5_s\\kendra\\babb\\4.wav  \n3051  cut_5_s\\matthew\\tele\\7.wav  \n1446  cut_5_s\\justin\\musi\\17.wav  \n3223    cut_5_s\\salli\\1-4\\33.wav  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>speaker</th>\n      <th>length</th>\n      <th>variant</th>\n      <th>segment</th>\n      <th>fullness</th>\n      <th>clarity</th>\n      <th>audio_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2174</th>\n      <td>kevin</td>\n      <td>5s</td>\n      <td>babble</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>cut_5_s\\kevin\\babb\\3.wav</td>\n    </tr>\n    <tr>\n      <th>1797</th>\n      <td>kendra</td>\n      <td>5s</td>\n      <td>babble</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>cut_5_s\\kendra\\babb\\4.wav</td>\n    </tr>\n    <tr>\n      <th>3051</th>\n      <td>matthew</td>\n      <td>5s</td>\n      <td>tv</td>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>cut_5_s\\matthew\\tele\\7.wav</td>\n    </tr>\n    <tr>\n      <th>1446</th>\n      <td>justin</td>\n      <td>5s</td>\n      <td>music</td>\n      <td>17</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>cut_5_s\\justin\\musi\\17.wav</td>\n    </tr>\n    <tr>\n      <th>3223</th>\n      <td>salli</td>\n      <td>5s</td>\n      <td>zoom_augmented</td>\n      <td>33</td>\n      <td>1</td>\n      <td>4</td>\n      <td>cut_5_s\\salli\\1-4\\33.wav</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the meta data about the synthetic speech dataset from the cache.\n",
    "df_synthetic_speech = pd.read_pickle(paths.cache.df_synthetic_speech)\n",
    "\n",
    "# Show the top 5 rows.\n",
    "display(df_synthetic_speech.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's play one of the first 5s segments from the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "    speaker length variant segment fullness clarity  \\\n559  joanna     5s  babble      10      NaN     NaN   \n\n                     audio_path  \n559  cut_5_s\\joanna\\babb\\10.wav  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>speaker</th>\n      <th>length</th>\n      <th>variant</th>\n      <th>segment</th>\n      <th>fullness</th>\n      <th>clarity</th>\n      <th>audio_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>559</th>\n      <td>joanna</td>\n      <td>5s</td>\n      <td>babble</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>cut_5_s\\joanna\\babb\\10.wav</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose which file to play.\n",
    "mask = (\n",
    "    (df_synthetic_speech['speaker'] == 'joanna')\n",
    "    & (df_synthetic_speech['length'] == '5s')\n",
    "    & (df_synthetic_speech['segment'] == 10)\n",
    "    & (df_synthetic_speech['variant'] == 'babble')\n",
    ")\n",
    "\n",
    "display(\n",
    "    df_synthetic_speech.loc[mask, :]\n",
    ")\n",
    "\n",
    "# Play.\n",
    "t.sound.play_synthetic_speech(df_synthetic_speech.loc[mask, :])"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "PyCharm (Exercises)",
   "language": "python",
   "name": "pycharm-f0629d26"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "445px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "093e89bffde74c5a8863a1d8ffe0bb66": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "1a5365fd770e464891fe02b94cb34b95": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1c7a498865af41a1bd4af63eca07bd28": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatTextModel",
      "state": {
       "layout": "IPY_MODEL_68759d18977d4757ba3eb3cf81aa1129",
       "step": null,
       "style": "IPY_MODEL_feea9000784940f3b601fd1ba613dd31"
      }
     },
     "1ee2806f2e314951b8b42666540f6aee": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "371339a327cc48d8a56358f512674975": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "65604f719f9e41f3b6d495d3aa907529": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "68759d18977d4757ba3eb3cf81aa1129": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "68dd48c190fe4b8c92790dc65803d785": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DropdownModel",
      "state": {
       "_options_labels": [
        "One",
        "Two",
        "Three"
       ],
       "description": "Number:",
       "index": 1,
       "layout": "IPY_MODEL_371339a327cc48d8a56358f512674975",
       "style": "IPY_MODEL_89e645fe88da40b2890bc1c08b50be78"
      }
     },
     "839595af423541ef946ce08e437e5b7e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "89e645fe88da40b2890bc1c08b50be78": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "9091258d01434bf8ac0f41c12363e6a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatTextModel",
      "state": {
       "layout": "IPY_MODEL_c22c18c67fe24f06ada7d28e3cb6ff90",
       "step": null,
       "style": "IPY_MODEL_839595af423541ef946ce08e437e5b7e"
      }
     },
     "afadc0b9a4f3458685319c1818955895": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DropdownModel",
      "state": {
       "_options_labels": [
        "One",
        "Two",
        "Three"
       ],
       "description": "Number:",
       "index": 1,
       "layout": "IPY_MODEL_1ee2806f2e314951b8b42666540f6aee",
       "style": "IPY_MODEL_093e89bffde74c5a8863a1d8ffe0bb66"
      }
     },
     "c22c18c67fe24f06ada7d28e3cb6ff90": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c64dd728486c4d4396b6b2b4a02ca054": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatSliderModel",
      "state": {
       "layout": "IPY_MODEL_1a5365fd770e464891fe02b94cb34b95",
       "step": 0.1,
       "style": "IPY_MODEL_ff2f617f26294276834c51733d0f253c"
      }
     },
     "d7c60be1aeb643fe9d80a622e61b4cc2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fe21ecb1be5e431c9745cf04421dc9a0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatSliderModel",
      "state": {
       "layout": "IPY_MODEL_d7c60be1aeb643fe9d80a622e61b4cc2",
       "step": 0.1,
       "style": "IPY_MODEL_65604f719f9e41f3b6d495d3aa907529",
       "value": 55.4
      }
     },
     "feea9000784940f3b601fd1ba613dd31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "ff2f617f26294276834c51733d0f253c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}