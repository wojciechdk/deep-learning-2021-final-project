{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "<span style=\"font-family:Lucida Bright;\">\n",
    "<p style=\"margin-bottom:0.5cm\"></p>\n",
    "<center>\n",
    "<font size=\"8\"><b>Deep Learning, Fall 2021</b></font>\n",
    "<p style=\"margin-bottom:0.6cm\"></p>\n",
    "<font size=\"3\"><b>Final Project:</b></font>\n",
    "<p style=\"margin-bottom:0.6cm\"></p>\n",
    "<font size=\"5\"><b>Enhancing Voices for Better Speech Intelligibility</b></font>\n",
    "<p style=\"margin-bottom:2cm\"></p>\n",
    "<font size=\"6\"><b>Start</b></font>\n",
    "</center>\n",
    "<p style=\"margin-bottom:2cm\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#How-to-get-the-most-out-of-this-notebook\" data-toc-modified-id=\"How-to-get-the-most-out-of-this-notebook-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>How to get the most out of this notebook</a></span></li><li><span><a href=\"#Prerequisites\" data-toc-modified-id=\"Prerequisites-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Prerequisites</a></span></li><li><span><a href=\"#Links-and-resources\" data-toc-modified-id=\"Links-and-resources-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Links and resources</a></span></li></ul></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Initialization\" data-toc-modified-id=\"Initialization-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Initialization</a></span></li><li><span><a href=\"#Load-data\" data-toc-modified-id=\"Load-data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Load data</a></span><ul class=\"toc-item\"><li><span><a href=\"#TIMIT-dataset\" data-toc-modified-id=\"TIMIT-dataset-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>TIMIT dataset</a></span></li><li><span><a href=\"#Synthetic-speech-dataset\" data-toc-modified-id=\"Synthetic-speech-dataset-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Synthetic speech dataset</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Many people struggle to understand speech in challenging acoustic environments, such as noisy bar. Therefore, enhancing the intelligibility of noisy speech signals is one of the key challenges for any producer of modern communication devices.\n",
    "\n",
    "The problem is often tackled by dividing a noisy speech signal into a number of frequency bands and attenuating the ones where the signal-to-noise ratio is insufficient. This approach, while effective in some situations, often leads to poor results, and sometimes even exacerbates the problem it is trying to solve as the constant activation and disactivation of some of the frequency bands in response to the fluctuations in speech and noise can create a very unnatural and disturbing sounds.\n",
    "\n",
    "In this project, we will try a different approach and attempt to create a deep learning model that will produce an equalization curve that can be applied to the noisy speech signal in order to maximize its intelligibility. This will be done by running a clear speech signal through a model of human auditory processing of and searching for a combination of parameters that produce a frequency-gain curve that, when applied to the noisy signal, creates the output most similar to that of clear speech."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Official project description"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the info-doc for the Deep Learning course, the project is described as follows:\n",
    "\n",
    "> **Designing self-driving earbuds with [augmentedhearing.io](augmentedhearing.io) which enhance voices based on function correlated with speech intelligibility**\n",
    ">\n",
    "> As one in four adults struggle to understand speech in challenging acoustics we aim to train consumer earbuds to enhance voices through back propagation using DHASP model implemented using [PyTorch differentiation package](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9414571) validated on [TIMIT speech dataset](https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3) based on an objective function correlated with [HASPI speech intelligibility auditory processing model](https://www.sciencedirect.com/science/article/pii/S0167639320300431) available in Matlab."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The approach"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The time scope for the course is normed to 7 days of 9 hours, which amounts to 63 hours. The outcome should be documented in a report formatted as a [conference paper](https://drive.google.com/file/d/0BxJRy96AHCJxaUEwOFhwUExmX00/view?usp=sharing&resourcekey=0-RvwJqDVrZVijbkkifLWoYA), as well as a Jupyter notebook that ideally should recreate the main results of the report.\n",
    "\n",
    "At the beginning of the project, the following resources were available:\n",
    "\n",
    "1. speech data: a TIMIT dataset consisting of 10 sentences spoken by 630 speakers from 8 major dialect regions of the United States\n",
    "2. an [article](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9414571) describing a proposal for a differentiable objective function that can be used to train a neural network. The function is a measure of similarity of a [cepstrum](https://en.wikipedia.org/wiki/Cepstrum) of a given speech signal to the cepstrum of the target signal (usually clear, noisless speech)\n",
    "3. Matlab code for calculating the [HASPI speech intelligibility index] that can be used to evaluate the results.\n",
    "\n",
    "In turn, to create a our model, we need the following:\n",
    "\n",
    "1. clear speech audio data to use as the target for model\n",
    "2. the corresponding noisy speech audio data to train the model\n",
    "3. a PyTorch implementation of the equalization filter that can be applied to the noisy speech signal. Our model will optimize the parameters of this filter to maximize speech intelligibility\n",
    "4. a working PyTorch implementation of the objective function proposed in the [DHASP article](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9414571).\n",
    "\n",
    "Therefore, before we could start creating and tweaking our neural network, we needed to obtain the prerequisites 2 - 4, of which especially number 4: implementation of the objective function proposed in the [DHASP article](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9414571) was, as I will demonstrate later, all but straightforward.\n",
    "\n",
    "To obtain a fully functional model would require going through and understanding the project literature, obtaining the prerequisites, crating and optimizing the neural network, and documenting of the findings, which is a task that extends way beyond the 63-hour scope of this project. I have dedicated more than 3 times as much time, and concentrated my effort on obtaining a functioning implementation of the all the prerequisites necessary to build a neural network that can be used to optimize our objective. Judged by the results I will present in this notebook, I might have succeeded."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a part of project. To be able to execute all of its content:\n",
    "\n",
    "1. Download the project repository: `https://github.com/wojciechdk/deep-learning-2021-final-project.git` and run the notebook from the root.\n",
    "2. Install the necessary packages specified in the file `[project_root]/requirements.txt`. It can be done in one hook by running the command `pip install -r requirements.txt` from the command prompt from the project root.\n",
    "3. Download the [TIMIT dataset](https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3), unpack it, and place it so that the folders *DOC*, *TEST*, and *TRAIN* are placed in the folder `[project_root]/resources/data/TIMIT`.\n",
    "4. Download the Sythetic Speech dataset created by Pawel Maciej Darulewski (for permission, please contact Pawel at s200123@student.dtu.dk). Place the data from the folder containing full length sentences in the folder `[project_root]/resources/data/synthetic_speech/full_length` and the data from the folder containing 5s segments in the folder `[project_root]/resources/data/synthetic_speech/cut_5_s`.\n",
    "\n",
    "Furthermore, to fully enjoy the content, please take note of the following:\n",
    "\n",
    "- The outputs of pre-executed cells may not be rendered properly unless the notebook is **Trusted**.\n",
    "- To avoid accidental changes, most of the cells in this notebook are marked-as read only, and many are frozen (i.e. disabled from being run). To take advantage of these features, it is recommended to use the extension\n",
    "[Freeze Cell](https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions/freeze/readme.html) which works with Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The organization of the project repository"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Functions and modules"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To keep the code clean, all the functions created by the author in the course of this project are defined in the modules of the package `toolbox` residing the folder `[project_root]/toolbox`. Each module contains the functions belonging to the category indicated by the name of the module.\n",
    "\n",
    "The following modules are used in the project:\n",
    "\n",
    "- `initialization`: contains all the code that needs to be executed before anything else, such as imports of necessary packages, setting of options, definition of project paths, etc.\n",
    "- `imports`: contains the imports of all the packages needed in this project.\n",
    "- `configuration`: contains the code that defines the options regarding the appearance and interactivity of the Jupyter Notebook, Pandas, etc.\n",
    "- `paths`: contains a class containing all the paths necessary to run this project.\n",
    "- `data_loading`: contains the functions that help load the data into meaningful structures, such as the functions that load all the metadata about the TIMIT and Synthetic Speech datasets into respective Pandas dataframes.\n",
    "- `dhasp`: contains the class containing a PyTorch implementation of the Differentiable Hearing Aid Speech Processing (DHASP) model described in this [article](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9414571)\n",
    "- `dsp`: contains the functions used for processing of signals and extracting their metrics.\n",
    "- `plotting`: contains the functions that help plot the data in this project.\n",
    "- `sound`: contains the functions make it easy to listen to the audio used in this project.\n",
    "- `type_conversion`: contains the functions that facilitate the conversion between different data types, e.g. numpy and a torch.\n",
    "- `general`: contains the functions that were not given a category of their own."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test scripts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All the scripts used for testing the code in produced in this project are placed in the folder: `[project_root]/tests`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Resources"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The project's resources are placed in the folder `[project_root]/resources` and include:\n",
    "- `cache`: the cached data, such as the metadata about the TIMIT and Synthetic Speech dataset.\n",
    "- `data`: the audio files for the TIMIT and Synthetic Speech dataset.\n",
    "- `matlab`: the Matlab code containing the functions used for calculation of the [HASPI speech intelligibility auditory processing model](https://www.sciencedirect.com/science/article/pii/S0167639320300431)\n",
    "- `stoi_examples`: contains a Jupyter Notebook containing examples of how to use a Python package to calculate speech intelligibility metrics alternative to HASPI."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The entire initialization process, including:\n",
    " - imports of the necessary packages\n",
    " - configuration of the notebook and packages\n",
    " - imports of the toolbox functions\n",
    "\n",
    "is defined in the file `[project_root]/toolbox/initialization.py`. Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from toolbox.initialization import *"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Everything is now initialized and the project paths are available in the variable `paths`. Let's view one path:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "WindowsPath('G:/My Drive/DTU/Kurser/Deep_Learning_02456/final_project/resources/data/TIMIT')"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paths.data.timit.root)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load information about the audio data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "At the beginning of the project, the only audio data I had at my disposal was the [TIMIT dataset](https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3). This dataset consists of 10 sentences spoken by 630 speakers from 8 major dialect regions of the United States. However, the data contains only recordings containing clean speech, while our objective function which measures similarity between the [cepstral](https://en.wikipedia.org/wiki/Cepstrum) sequences for two audio signals, requires both clean and noisy versions of the same speech segment.\n",
    "\n",
    "Instead of generating the noisy data, I was offered by Pawel Maciej Darulewski to use a set containing samples of synthetically generated speech in different acoustical situations, which he has created for a similar project. In the following, I will therefore use Pawel's audio data. I have, however, implemented functions that allow easy access to both the TIMIT and Pawel's Synthetic Speech datasets, which I will present in the following sections.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## TIMIT dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The metadata about the TIMIT dataset is loaded using the function `load_timit_data` defined in the module `[project_root]\\toolbox\\data_loading.py`. Once loaded, the data is saved in the project cache as a Pandas dataframe. Let's load it and show a couple of rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     sentence_number data_group        dialect gender speaker  \\\n5243            1754       test  North Midland      M    WJG0   \n2297             344      train  South Midland      M    JSR0   \n2176             255      train  South Midland      M    GJC0   \n2530               1      train  South Midland      M    STF0   \n627              347      train       Northern      M    BJV0   \n\n                      type                                               text  \\\n5243  phonetically-diverse               He said: the crazy fool, half aloud.   \n2297  phonetically-compact             I know I didn't meet her early enough.   \n2176  phonetically-compact           My ideal morning begins with hot coffee.   \n2530               dialect  She had your dark suit in greasy wash water al...   \n627   phonetically-compact  The patient and the surgeon are both recuperat...   \n\n                     audio_path start_sample end_sample  \\\n5243  test\\DR3\\MWJG0\\SI1754.wav            0      42292   \n2297  train\\DR4\\MJSR0\\SX344.wav            0      48845   \n2176  train\\DR4\\MGJC0\\SX255.wav            0      49357   \n2530    train\\DR4\\MSTF0\\SA1.wav            0      55296   \n627   train\\DR2\\MBJV0\\SX347.wav            0      57856   \n\n                                             words_text  \\\n5243          [he, said, the, crazy, fool, half, aloud]   \n2297     [i, know, i, didn't, meet, her, early, enough]   \n2176    [my, ideal, morning, begins, with, hot, coffee]   \n2530  [she, had, your, dark, suit, in, greasy, wash,...   \n627   [the, patient, and, the, surgeon, are, both, r...   \n\n                                     words_start_sample  \\\n5243    [2040, 3480, 15791, 17320, 23400, 32040, 35759]   \n2297  [2372, 4206, 8355, 10120, 15940, 20640, 27708,...   \n2176    [2372, 6858, 15164, 22308, 32233, 35108, 40080]   \n2530  [2190, 4840, 8560, 10360, 15624, 25200, 27520,...   \n627   [1851, 2719, 10253, 12040, 13080, 19640, 21320...   \n\n                                       words_end_sample  \\\n5243   [3480, 11446, 17320, 23400, 29880, 35759, 41420]   \n2297  [4206, 8355, 10120, 15940, 20640, 25733, 32793...   \n2176   [5880, 15164, 22308, 31201, 35108, 40080, 46633]   \n2530  [4840, 9160, 10360, 15624, 23310, 27520, 33921...   \n627   [2719, 10253, 12040, 13080, 19640, 21320, 2596...   \n\n                                          phonemes_text  \\\n5243  [h#, hh, iy, s, eh, dcl, pau, dh, ix, kcl, k, ...   \n2297  [h#, ay, n, ow, ay, dcl, d, ih, dcl, en, m, iy...   \n2176  [h#, m, ay, q, ay, dcl, d, ih, l, m, ao, r, nx...   \n2530  [h#, s, iy, hv, ae, dcl, jh, axr, dcl, d, aa, ...   \n627   [h#, dh, ax, pcl, p, ey, sh, ix, n, tcl, t, eh...   \n\n                                  phonemes_start_sample  \\\n5243  [0, 2040, 2680, 3480, 5800, 8280, 11446, 15791...   \n2297  [0, 2372, 4206, 5450, 8355, 10120, 11520, 1168...   \n2176  [0, 2372, 3520, 5880, 6858, 10109, 11655, 1182...   \n2530  [0, 2190, 3870, 4840, 5480, 7960, 8560, 9160, ...   \n627   [0, 1851, 2356, 2719, 3640, 4440, 6360, 7690, ...   \n\n                                    phonemes_end_sample  \n5243  [2040, 2680, 3480, 5800, 8280, 11446, 15791, 1...  \n2297  [2372, 4206, 5450, 8355, 10120, 11520, 11680, ...  \n2176  [2372, 3520, 5880, 6858, 10109, 11655, 11822, ...  \n2530  [2190, 3870, 4840, 5480, 7960, 8560, 9160, 103...  \n627   [1851, 2356, 2719, 3640, 4440, 6360, 7690, 818...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence_number</th>\n      <th>data_group</th>\n      <th>dialect</th>\n      <th>gender</th>\n      <th>speaker</th>\n      <th>type</th>\n      <th>text</th>\n      <th>audio_path</th>\n      <th>start_sample</th>\n      <th>end_sample</th>\n      <th>words_text</th>\n      <th>words_start_sample</th>\n      <th>words_end_sample</th>\n      <th>phonemes_text</th>\n      <th>phonemes_start_sample</th>\n      <th>phonemes_end_sample</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5243</th>\n      <td>1754</td>\n      <td>test</td>\n      <td>North Midland</td>\n      <td>M</td>\n      <td>WJG0</td>\n      <td>phonetically-diverse</td>\n      <td>He said: the crazy fool, half aloud.</td>\n      <td>test\\DR3\\MWJG0\\SI1754.wav</td>\n      <td>0</td>\n      <td>42292</td>\n      <td>[he, said, the, crazy, fool, half, aloud]</td>\n      <td>[2040, 3480, 15791, 17320, 23400, 32040, 35759]</td>\n      <td>[3480, 11446, 17320, 23400, 29880, 35759, 41420]</td>\n      <td>[h#, hh, iy, s, eh, dcl, pau, dh, ix, kcl, k, ...</td>\n      <td>[0, 2040, 2680, 3480, 5800, 8280, 11446, 15791...</td>\n      <td>[2040, 2680, 3480, 5800, 8280, 11446, 15791, 1...</td>\n    </tr>\n    <tr>\n      <th>2297</th>\n      <td>344</td>\n      <td>train</td>\n      <td>South Midland</td>\n      <td>M</td>\n      <td>JSR0</td>\n      <td>phonetically-compact</td>\n      <td>I know I didn't meet her early enough.</td>\n      <td>train\\DR4\\MJSR0\\SX344.wav</td>\n      <td>0</td>\n      <td>48845</td>\n      <td>[i, know, i, didn't, meet, her, early, enough]</td>\n      <td>[2372, 4206, 8355, 10120, 15940, 20640, 27708,...</td>\n      <td>[4206, 8355, 10120, 15940, 20640, 25733, 32793...</td>\n      <td>[h#, ay, n, ow, ay, dcl, d, ih, dcl, en, m, iy...</td>\n      <td>[0, 2372, 4206, 5450, 8355, 10120, 11520, 1168...</td>\n      <td>[2372, 4206, 5450, 8355, 10120, 11520, 11680, ...</td>\n    </tr>\n    <tr>\n      <th>2176</th>\n      <td>255</td>\n      <td>train</td>\n      <td>South Midland</td>\n      <td>M</td>\n      <td>GJC0</td>\n      <td>phonetically-compact</td>\n      <td>My ideal morning begins with hot coffee.</td>\n      <td>train\\DR4\\MGJC0\\SX255.wav</td>\n      <td>0</td>\n      <td>49357</td>\n      <td>[my, ideal, morning, begins, with, hot, coffee]</td>\n      <td>[2372, 6858, 15164, 22308, 32233, 35108, 40080]</td>\n      <td>[5880, 15164, 22308, 31201, 35108, 40080, 46633]</td>\n      <td>[h#, m, ay, q, ay, dcl, d, ih, l, m, ao, r, nx...</td>\n      <td>[0, 2372, 3520, 5880, 6858, 10109, 11655, 1182...</td>\n      <td>[2372, 3520, 5880, 6858, 10109, 11655, 11822, ...</td>\n    </tr>\n    <tr>\n      <th>2530</th>\n      <td>1</td>\n      <td>train</td>\n      <td>South Midland</td>\n      <td>M</td>\n      <td>STF0</td>\n      <td>dialect</td>\n      <td>She had your dark suit in greasy wash water al...</td>\n      <td>train\\DR4\\MSTF0\\SA1.wav</td>\n      <td>0</td>\n      <td>55296</td>\n      <td>[she, had, your, dark, suit, in, greasy, wash,...</td>\n      <td>[2190, 4840, 8560, 10360, 15624, 25200, 27520,...</td>\n      <td>[4840, 9160, 10360, 15624, 23310, 27520, 33921...</td>\n      <td>[h#, s, iy, hv, ae, dcl, jh, axr, dcl, d, aa, ...</td>\n      <td>[0, 2190, 3870, 4840, 5480, 7960, 8560, 9160, ...</td>\n      <td>[2190, 3870, 4840, 5480, 7960, 8560, 9160, 103...</td>\n    </tr>\n    <tr>\n      <th>627</th>\n      <td>347</td>\n      <td>train</td>\n      <td>Northern</td>\n      <td>M</td>\n      <td>BJV0</td>\n      <td>phonetically-compact</td>\n      <td>The patient and the surgeon are both recuperat...</td>\n      <td>train\\DR2\\MBJV0\\SX347.wav</td>\n      <td>0</td>\n      <td>57856</td>\n      <td>[the, patient, and, the, surgeon, are, both, r...</td>\n      <td>[1851, 2719, 10253, 12040, 13080, 19640, 21320...</td>\n      <td>[2719, 10253, 12040, 13080, 19640, 21320, 2596...</td>\n      <td>[h#, dh, ax, pcl, p, ey, sh, ix, n, tcl, t, eh...</td>\n      <td>[0, 1851, 2356, 2719, 3640, 4440, 6360, 7690, ...</td>\n      <td>[1851, 2356, 2719, 3640, 4440, 6360, 7690, 818...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the TIMIT meta data from the cache.\n",
    "df_timit = pd.read_pickle(paths.cache.df_timit)\n",
    "\n",
    "# Show the top 5 rows\n",
    "display(df_timit.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, let's play one sentence from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t.sound.play_timit(df_timit.loc[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Synthetic speech dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The metadata about the dataset containing synthetic speech in different audio settings is loaded using the function `load_synthetic_speech_data` defined in the module `toolbox\\data_loading.py`. Once loaded, the data is saved in the project cache as a Pandas dataframe. Let's load it and show a couple of rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       speaker length         variant segment fullness clarity  \\\n2282     kevin     5s              tv       6      NaN     NaN   \n2106     kevin     5s  zoom_augmented      19        1       4   \n2779  kimberly     5s           clean      30      NaN     NaN   \n789       joey     5s  zoom_augmented      11        1       1   \n429     joanna     5s  zoom_augmented      31        1       1   \n\n                        audio_path  \n2282      cut_5_s\\kevin\\tele\\6.wav  \n2106      cut_5_s\\kevin\\1-4\\19.wav  \n2779  cut_5_s\\kimberly\\orig\\30.wav  \n789        cut_5_s\\joey\\1-1\\11.wav  \n429      cut_5_s\\joanna\\1-1\\31.wav  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>speaker</th>\n      <th>length</th>\n      <th>variant</th>\n      <th>segment</th>\n      <th>fullness</th>\n      <th>clarity</th>\n      <th>audio_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2282</th>\n      <td>kevin</td>\n      <td>5s</td>\n      <td>tv</td>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>cut_5_s\\kevin\\tele\\6.wav</td>\n    </tr>\n    <tr>\n      <th>2106</th>\n      <td>kevin</td>\n      <td>5s</td>\n      <td>zoom_augmented</td>\n      <td>19</td>\n      <td>1</td>\n      <td>4</td>\n      <td>cut_5_s\\kevin\\1-4\\19.wav</td>\n    </tr>\n    <tr>\n      <th>2779</th>\n      <td>kimberly</td>\n      <td>5s</td>\n      <td>clean</td>\n      <td>30</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>cut_5_s\\kimberly\\orig\\30.wav</td>\n    </tr>\n    <tr>\n      <th>789</th>\n      <td>joey</td>\n      <td>5s</td>\n      <td>zoom_augmented</td>\n      <td>11</td>\n      <td>1</td>\n      <td>1</td>\n      <td>cut_5_s\\joey\\1-1\\11.wav</td>\n    </tr>\n    <tr>\n      <th>429</th>\n      <td>joanna</td>\n      <td>5s</td>\n      <td>zoom_augmented</td>\n      <td>31</td>\n      <td>1</td>\n      <td>1</td>\n      <td>cut_5_s\\joanna\\1-1\\31.wav</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the meta data about the synthetic speech dataset from the cache.\n",
    "df_synthetic_speech = pd.read_pickle(paths.cache.df_synthetic_speech)\n",
    "\n",
    "# Show the top 5 rows.\n",
    "display(df_synthetic_speech.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's play one of the first 5s segments from the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "    speaker length variant segment fullness clarity  \\\n559  joanna     5s  babble      10      NaN     NaN   \n\n                     audio_path  \n559  cut_5_s\\joanna\\babb\\10.wav  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>speaker</th>\n      <th>length</th>\n      <th>variant</th>\n      <th>segment</th>\n      <th>fullness</th>\n      <th>clarity</th>\n      <th>audio_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>559</th>\n      <td>joanna</td>\n      <td>5s</td>\n      <td>babble</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>cut_5_s\\joanna\\babb\\10.wav</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the properties of the file to play.\n",
    "mask = (\n",
    "        (df_synthetic_speech['speaker'] == 'joanna')\n",
    "        & (df_synthetic_speech['length'] == '5s')\n",
    "        & (df_synthetic_speech['segment'] == 10)\n",
    "        & (df_synthetic_speech['variant'] == 'babble')\n",
    ")\n",
    "\n",
    "# Show the file data.\n",
    "display(\n",
    "    df_synthetic_speech.loc[mask, :]\n",
    ")\n",
    "\n",
    "# Play.\n",
    "t.sound.play_synthetic_speech(df_synthetic_speech.loc[mask, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Implementation of the DHASP model as the objective function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is a declared goal of the project to base the objective function of the model on the [Differentiable Hearing Aid Speech Processing (DHASP)](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9414571) framework. This framework has been developed to optimize the signal processing in a hearing aid so that the signal perceived by a hearing-impaired person would be as close as possibe to that perceived by a person with normal hearing.\n",
    "\n",
    "<center>\n",
    "<br>\n",
    "<img src=\"resources/graphics/dhasp_original_framework.jpg\"\n",
    "alt=\"DHASP framework as proposed by its creators\"\n",
    "title=\"Original DHASP framework\"\n",
    "width=\"600\"/>\n",
    "<br>\n",
    "</center>\n",
    "\n",
    "The original framework compares therefore the output of auditory processing of a signal by a person with normal hearing to the output of the processing of **the same** signal by a person with impaired hearing (figure XXX). We, on the other hand, would like to alter this framework so that it compares the result of auditory processing of two **different** signals - noisy and noise-free - by the same, normal hearing person (Figure XXX). When constructed this way, the framework can tweak the equalization of the noisy signal so that it resembles its noise-free counterpart to a highest possible degree.\n",
    "\n",
    "<center>\n",
    "<br>\n",
    "<img src=\"resources/graphics/dhasp_proposed_framework.jpg\"\n",
    "alt=\"DHASP framework as proposed by its creators\"\n",
    "title=\"Original DHASP framework\"\n",
    "width=\"600\"/>\n",
    "<br>\n",
    "</center>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The EQ"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I have chosen that the EQ applied to the noisy speech with the goal of improving its intelligibility will consist of 8 fourth order gammatone bandpass filters, whose:\n",
    " - center frequencies are logarithmically spaced between 100 Hz and 8000 Hz\n",
    " - bandwidths are set to their center frequencies divided by 2.3\n",
    " - peak gain is set to unity, i.e. 0 dB.\n",
    "\n",
    "Summarising, the coefficients of the $i^{\\text{th}}$ filter are given by:\n",
    "\n",
    "$$\n",
    "\\mathbf{h_{\\text{eq}}^{(i)}}[n] =\n",
    "c_{\\text{norm}}^{(i)}\n",
    "\\left(  \\frac {n} {f_s}  \\right) ^ {(N - 1)}\n",
    "\\exp \\left(  -2 \\pi b_{\\text{eq}}^{(i)} \\frac {n} {f_s}  \\right)\n",
    "\\cos \\left(  2 \\pi f_{\\text{eq}}  \\frac {n} {f_s}   \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $n$ is the sample number\n",
    "- $f_s$ is the sampling frequency\n",
    "- $f_{\\text{eq}}^{(i)}$ is the center frequency of the $i^{\\text{th}}$ filter\n",
    "- $b_{\\text{eq}}^{(i)}$ is the bandwidth of the filter.\n",
    "- $N$ is the order of the *gammatone* filter, in our case set to 4\n",
    "- $c_{\\text{norm}}^{(i)}$ is a constant normalizing the peak gain of the $i^{\\text{th}}$ filter to 0 dB, i.e.:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "c_{\\text{norm}}^{(i)}\n",
    " &= \\left| \\sum_n  \\mathbf{h_{\\text{eq}}^{(i)}}[n] \\exp \\left(  -2j \\pi f_{\\text{eq}}^{(i)} \\frac {n} {f_s}   \\right) \\right| \\\\\n",
    " &= \\sqrt{\n",
    "  \\left( \\sum_n  \\mathbf{h_{\\text{eq}}^{(i)}}[n] \\cos \\left( 2 \\pi f_{\\text{eq}}^{(i)} \\frac {n} {f_s}  \\right)  \\right)^2\n",
    "   + \\left( \\sum_n  \\mathbf{h_{\\text{eq}}^{(i)}}[n] \\sin \\left( 2 \\pi f_{\\text{eq}}^{(i)} \\frac {n} {f_s}  \\right)  \\right)^2\n",
    "   }\n",
    "\\end{align}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "The total response of this filterbank, i.e. the sum of the outputs of its filters, is then given by:\n",
    "\n",
    "$$\n",
    "\\mathbf{h_{\\text{eq}}}[n] = \\sum_i \\mathbf{h_{\\text{eq}}^{(i)}}[n]\n",
    "$$\n",
    "\n",
    "Having defined the filters, we can assign a gain $g_i$ to each frequency band, apply the modified filter it to a speech signal $\\mathbf{x_i}$, and feed the result to our neural network in order to optimize the gains $g_i$ for best speech intelligibility by minimizing the loss function $L$:\n",
    "\n",
    "$$\n",
    "G_{\\text{optimized}} = \\underset{G}{\\text{argmin}} L \\left( f(G, X), Y \\right)\n",
    "$$\n",
    "\n",
    "where: $L$ is the loss function which we will define in the following chapters, $ G = \\left\\{ {g_1, g_2, \\cdots, g_8}  \\right\\}$ a matrix containing the gains for our EQ, $X = \\left\\{ \\mathbf{x_1}, \\mathbf{x_2}, \\cdots  \\right}$  matrix in which each row represents an speech signal, and $Y = \\left\\{ \\mathbf{y_1}, \\mathbf{y_2}, \\cdots  \\right}$ a matrix where each row represents a corresponding target (clear) speech signal. $f(G, \\mathbf{x})$ represents a speech signal after having been fitered with our EQ, i.e.:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(G, x)\n",
    "&= \\text{conv} \\left(  G  \\mathbf{h_{\\text{eq}}}, \\mathbf{x}  \\right) \\\\\n",
    "&= \\text{conv} \\left(  \\sum_i g_i \\mathbf{h_{\\text{eq}}^{(i)}}, \\mathbf{x}  \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The auditory model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The structure of the auditory model is shown in Figure XXX. The input signal first goes through two filterbanks: an analysis filterbank and a control filterbank, both consisting of 32 bandpass filters with varying properties. Based on the output of the control fliterbank, a dynamic-range compression gain is calculated for frequency band and applied to the outputs of the filters in the analysis filterbank. The output of the auditory model is then calculated as amplitude envelopes (in dB) of the compressed outputs of the filters in the analysis filterbank.\n",
    "\n",
    "<center>\n",
    "<br>\n",
    "<img src=\"resources/graphics/dhasp_differentiable_auditory_model.jpg\"\n",
    "alt=\"DHASP framework as proposed by its creators\"\n",
    "title=\"Original DHASP framework\"\n",
    "width=\"600\"/>\n",
    "<br>\n",
    "</center>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The filterbanks\n",
    "\n",
    "The coefficients of the filterbanks in the auditory model are calculated in the same way as the coefficients for the EQ applied to the noisy signal described in section XXX, i.e.:\n",
    "\n",
    "$$\n",
    "\\mathbf{h}[n] =\n",
    "c_{\\text{norm}}^{(i)}\n",
    "\\left(  \\frac {n} {f_s}  \\right) ^ {(N - 1)}\n",
    "\\exp \\left(  -2 \\pi b^{(i)} \\frac {n} {f}  \\right)\n",
    "\\cos \\left(  2 \\pi f  \\frac {n} {f_s}   \\right)\n",
    "$$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Center frequencies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "In the **analysis** filterbank, the center frequencies are in the Mel scale and cover the e range from 80 Hz to 8 kHz. To calculate them, I have created functions that convert Hz to Mel and vice versa. The computation is then conducted as follows:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_a[:3, :]=tensor([[ 80.0000],\n",
      "        [143.1066],\n",
      "        [211.3189]], dtype=torch.float64)\n",
      "f_a[-3:, :]=tensor([[6746.3509],\n",
      "        [7348.8016],\n",
      "        [7999.9987]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Define the number of filters: I.\n",
    "I = 32\n",
    "\n",
    "# Calculate the frequencies for the analysis filterbank.\n",
    "f_a = t.dsp.mel2hz(torch.linspace(\n",
    "    t.dsp.hz2mel(80),\n",
    "    t.dsp.hz2mel(8000),\n",
    "    I\n",
    ")).reshape(I, 1)\n",
    "\n",
    "# Show the first 3 and last 3 frequencies.\n",
    "print(f'{f_a[:3, :]=}')\n",
    "print(f'{f_a[-3:, :]=}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "In the **control** filterbank, the center frequencies are given by equation 3 in the [DHASP article](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9414571).\n",
    "\n",
    "$$\n",
    "f_{\\text{c}}^{(i)} =\n",
    "165.4\n",
    "\\left(\n",
    "    10^{\n",
    "        (1 + s)\n",
    "        \\log_{10}\n",
    "        \\left( 1 + f_{\\text{a}}^{(i)} / 165.4 \\right)\n",
    "        }\n",
    "    - 1\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Here, I have corrected a mistake in the paper, where $f_{\\text{a}}$ was replaced by $f_{\\text{c}}$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Bandwidths"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The bandwidths of the analysis filter are in the equivalent rectangular bandwidth (ERB) scale for the normal hearing model. The formula for calculating the ERB for a given frequency is taken from the paper: [Suggested formulae for calculating auditory-filter bandwidths and excitation patterns](https://pubmed.ncbi.nlm.nih.gov/6630731/). I have implemented the formula in the function `erb`, so that the bandwidths can be calculated as:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_a[:3, :]=tensor([[ 80.0000],\n",
      "        [143.1066],\n",
      "        [211.3189]], dtype=torch.float64)\n",
      "f_a[-3:, :]=tensor([[6746.3509],\n",
      "        [7348.8016],\n",
      "        [7999.9987]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the bandwidths for the analysis filterbank.\n",
    "b_a = t.dsp.erb(f_a)\n",
    "\n",
    "# Show the first 3 and last 3 bandwidths.\n",
    "print(f'{f_a[:3, :]=}')\n",
    "print(f'{f_a[-3:, :]=}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "[Suggested formulae for calculating auditory-filter bandwidths and excitation patterns](https://pubmed.ncbi.nlm.nih.gov/6630731/)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# End"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "PyCharm (Exercises)",
   "language": "python",
   "name": "pycharm-f0629d26"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "445px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "093e89bffde74c5a8863a1d8ffe0bb66": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "1a5365fd770e464891fe02b94cb34b95": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1c7a498865af41a1bd4af63eca07bd28": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatTextModel",
      "state": {
       "layout": "IPY_MODEL_68759d18977d4757ba3eb3cf81aa1129",
       "step": null,
       "style": "IPY_MODEL_feea9000784940f3b601fd1ba613dd31"
      }
     },
     "1ee2806f2e314951b8b42666540f6aee": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "371339a327cc48d8a56358f512674975": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "65604f719f9e41f3b6d495d3aa907529": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "68759d18977d4757ba3eb3cf81aa1129": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "68dd48c190fe4b8c92790dc65803d785": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DropdownModel",
      "state": {
       "_options_labels": [
        "One",
        "Two",
        "Three"
       ],
       "description": "Number:",
       "index": 1,
       "layout": "IPY_MODEL_371339a327cc48d8a56358f512674975",
       "style": "IPY_MODEL_89e645fe88da40b2890bc1c08b50be78"
      }
     },
     "839595af423541ef946ce08e437e5b7e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "89e645fe88da40b2890bc1c08b50be78": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "9091258d01434bf8ac0f41c12363e6a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatTextModel",
      "state": {
       "layout": "IPY_MODEL_c22c18c67fe24f06ada7d28e3cb6ff90",
       "step": null,
       "style": "IPY_MODEL_839595af423541ef946ce08e437e5b7e"
      }
     },
     "afadc0b9a4f3458685319c1818955895": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DropdownModel",
      "state": {
       "_options_labels": [
        "One",
        "Two",
        "Three"
       ],
       "description": "Number:",
       "index": 1,
       "layout": "IPY_MODEL_1ee2806f2e314951b8b42666540f6aee",
       "style": "IPY_MODEL_093e89bffde74c5a8863a1d8ffe0bb66"
      }
     },
     "c22c18c67fe24f06ada7d28e3cb6ff90": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c64dd728486c4d4396b6b2b4a02ca054": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatSliderModel",
      "state": {
       "layout": "IPY_MODEL_1a5365fd770e464891fe02b94cb34b95",
       "step": 0.1,
       "style": "IPY_MODEL_ff2f617f26294276834c51733d0f253c"
      }
     },
     "d7c60be1aeb643fe9d80a622e61b4cc2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fe21ecb1be5e431c9745cf04421dc9a0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatSliderModel",
      "state": {
       "layout": "IPY_MODEL_d7c60be1aeb643fe9d80a622e61b4cc2",
       "step": 0.1,
       "style": "IPY_MODEL_65604f719f9e41f3b6d495d3aa907529",
       "value": 55.4
      }
     },
     "feea9000784940f3b601fd1ba613dd31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "ff2f617f26294276834c51733d0f253c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}